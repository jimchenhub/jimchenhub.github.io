<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.6.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2018-03-08T00:11:30+08:00</updated><id>http://localhost:4000/</id><title type="html">Jim Chen</title><subtitle>Hello 911 emergency, there's a handsome guy in my house! Oh wait a second, cancel that. It's only me.
</subtitle><author><name>Jim Chen</name></author><entry><title type="html">关于Batch_Normalization的公式推导和代码实现</title><link href="http://localhost:4000/posts/2018-03-07-%E5%85%B3%E4%BA%8EBatch_Normalization%E7%9A%84%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC%E5%92%8C%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0" rel="alternate" type="text/html" title="关于Batch_Normalization的公式推导和代码实现" /><published>2018-03-07T20:55:00+08:00</published><updated>2018-03-07T20:55:00+08:00</updated><id>http://localhost:4000/posts/%E5%85%B3%E4%BA%8EBatch_Normalization%E7%9A%84%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC%E5%92%8C%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0</id><content type="html" xml:base="http://localhost:4000/posts/2018-03-07-%E5%85%B3%E4%BA%8EBatch_Normalization%E7%9A%84%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC%E5%92%8C%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0">&lt;p&gt;最近在学习CS231n，其中笔记部分会在其他部分，这里重点推导一下Batch Normalization的前馈和反馈。前馈部分比较简单，直接复制原文的图吧。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://kratzert.github.io/images/bn_backpass/bn_algorithm.PNG&quot; alt=&quot;BN forward&quot; /&gt;&lt;/p&gt;

&lt;p&gt;这里对前馈的部分做一下简要的说明。normalize部分是一个平移和缩放的组合，但是为了保证并非所有层都是零均值的unit gaussian。所以加上了后面的scale and shift部分，来增加BN层的灵活性。&lt;/p&gt;

&lt;p&gt;关于BN的方向传播，一种方法是使用computational graph来计算，这种方法主要参考 &lt;a href=&quot;https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html&quot;&gt;Understanding the backward pass through Batch Normalization Layer&lt;/a&gt;。对应的计算图如下&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://kratzert.github.io/images/bn_backpass/BNcircuit.png&quot; alt=&quot;computational graph&quot; /&gt;&lt;/p&gt;

&lt;p&gt;另一种反向传播的方法是直接计算出直接的求导式子。但是由于x hat对于x求导时无论mean还是variance都包含x，所以求导比较复杂，这里主要参考 &lt;a href=&quot;https://kevinzakka.github.io/2016/09/14/batch_normalization/&quot;&gt;Deriving the Gradient for the Backward Pass of Batch Normalization&lt;/a&gt;。其中对于x的求导是计算多个部分的偏导然后求和计算的。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/bn_backward_formulations.png&quot; alt=&quot;bn_backward_formulations&quot; /&gt;&lt;/p&gt;</content><author><name>Jim Chen</name></author><summary type="html">最近在学习CS231n，其中笔记部分会在其他部分，这里重点推导一下Batch Normalization的前馈和反馈。前馈部分比较简单，直接复制原文的图吧。</summary></entry><entry><title type="html">ROS学习Navigation（二）</title><link href="http://localhost:4000/posts/2017-08-11-ROS%E5%AD%A6%E4%B9%A0Navigation(%E4%BA%8C)" rel="alternate" type="text/html" title="ROS学习Navigation（二）" /><published>2017-08-11T19:30:00+08:00</published><updated>2017-08-11T19:30:00+08:00</updated><id>http://localhost:4000/posts/ROS%E5%AD%A6%E4%B9%A0Navigation(%E4%BA%8C)</id><content type="html" xml:base="http://localhost:4000/posts/2017-08-11-ROS%E5%AD%A6%E4%B9%A0Navigation(%E4%BA%8C)">&lt;p&gt;紧接着上一篇继续实现第二部分。&lt;/p&gt;

&lt;h2 id=&quot;setup-and-configuration-of-the-navigation-stack-on-a-robot&quot;&gt;Setup and Configuration of the Navigation Stack on a Robot&lt;/h2&gt;

&lt;p&gt;在学习了基础的TF知识后，可以开始逐步学习如何publish tf的数据，Odometry的数据，sensor的数据，以及基础的navigation的设置。官方网站上写明了一个基础的navigation的tuning方法的&lt;a href=&quot;http://wiki.ros.org/navigation/Tutorials/Navigation%20Tuning%20Guide&quot;&gt;教程&lt;/a&gt;，其中的cost map和Local Planer的部分是讲得稍微深入具体一些的，还没有完全看懂，但是相信日后会比较有参考价值。&lt;/p&gt;

&lt;p&gt;首先来看一个整体的设计图，这个图非常的高层抽象，是很好的理解材料&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://wiki.ros.org/navigation/Tutorials/RobotSetup?action=AttachFile&amp;amp;do=get&amp;amp;target=overview_tf_small.png&quot; alt=&quot;robot setup&quot; /&gt;&lt;/p&gt;

&lt;p&gt;白色的框是已经被实现的必须的部分，灰色的已经被实现的可选部分，蓝色的是需要对不同机器人设计的部分。&lt;/p&gt;

&lt;h3 id=&quot;1-setup&quot;&gt;1. Setup&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;首先需要完成TF transform information 信息的发布，这个在前一大节已经讲解。&lt;/li&gt;
  &lt;li&gt;机器人在世界中的导航是需要获取传感器信息来避障的，所以ROS假设这些信息是通过&lt;code class=&quot;highlighter-rouge&quot;&gt;sensor_msgs/LaserScan&lt;/code&gt; 或 &lt;code class=&quot;highlighter-rouge&quot;&gt;sensor_msgs/PointCloud&lt;/code&gt;来接收的。&lt;a href=&quot;http://wiki.ros.org/navigation/Tutorials/RobotSetup/Sensors&quot;&gt;Publishing Sensor Streams Over ROS&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Odometry信息需要通过&lt;code class=&quot;highlighter-rouge&quot;&gt;tf&lt;/code&gt;和 &lt;code class=&quot;highlighter-rouge&quot;&gt;nav_msgs/Odometry&lt;/code&gt;来发布。&lt;a href=&quot;http://wiki.ros.org/navigation/Tutorials/RobotSetup/Odom&quot;&gt;这里&lt;/a&gt;是有相关的教程的。&lt;/li&gt;
  &lt;li&gt;Base Controller (base controller)。速度的指令是通过&lt;code class=&quot;highlighter-rouge&quot;&gt;geometry_msgs/Twist&lt;/code&gt; message来传递给&lt;code class=&quot;highlighter-rouge&quot;&gt;cmd_vel&lt;/code&gt; topic的。使用的frame是&lt;code class=&quot;highlighter-rouge&quot;&gt;base coordinate frame&lt;/code&gt;。&lt;/li&gt;
  &lt;li&gt;Mapping (map_server) navigation本身是不需要map的，但是可以在 &lt;a href=&quot;http://wiki.ros.org/slam_gmapping/Tutorials/MappingFromLoggedData&quot;&gt;building a map&lt;/a&gt; 来创建一个运行环境的map。&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;2-navigation-stack-setup&quot;&gt;2. Navigation Stack Setup&lt;/h3&gt;

&lt;p&gt;至此，已经通过tf publish了坐标系frame，可以接收到&lt;code class=&quot;highlighter-rouge&quot;&gt;sensor_msgs/LaserScan&lt;/code&gt; 或 &lt;code class=&quot;highlighter-rouge&quot;&gt;sensor_msgs/PointCloud&lt;/code&gt;的sensor信息，使用了tf和&lt;code class=&quot;highlighter-rouge&quot;&gt;nav_msgs/Odometry&lt;/code&gt;发布了Odometry信息，同时接收了速度的命令。如果有任何一条未能满足，需要先完成上述的Setup部分。&lt;/p&gt;

&lt;h4 id=&quot;21-创建package&quot;&gt;2.1 创建package&lt;/h4&gt;
&lt;p&gt;这个package要依赖于前面的所有package以及包含navigation stack高层接口 move_base package。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;catkin_create_pkg my_robot_name_2dnav roscpp move_base my_tf_configuration_dep my_odom_configuration_dep my_sensor_configuration_dep
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;my_tf_configuration_dep&lt;/code&gt; 包括&lt;code class=&quot;highlighter-rouge&quot;&gt;tf&lt;/code&gt;和&lt;code class=&quot;highlighter-rouge&quot;&gt; geometry_msgs&lt;/code&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;my_odom_configuration_dep &lt;/code&gt; 包括&lt;/p&gt;</content><author><name>Jim Chen</name></author><summary type="html">紧接着上一篇继续实现第二部分。</summary></entry><entry><title type="html">ROS学习Navigation（一）</title><link href="http://localhost:4000/posts/2017-08-07-ROS%E5%AD%A6%E4%B9%A0Navigation(%E4%B8%80)" rel="alternate" type="text/html" title="ROS学习Navigation（一）" /><published>2017-08-07T16:30:00+08:00</published><updated>2017-08-07T16:30:00+08:00</updated><id>http://localhost:4000/posts/ROS%E5%AD%A6%E4%B9%A0Navigation(%E4%B8%80)</id><content type="html" xml:base="http://localhost:4000/posts/2017-08-07-ROS%E5%AD%A6%E4%B9%A0Navigation(%E4%B8%80)">&lt;p&gt;这次不想从头开始写ROS学习的部分，基础的很多概念都是学习的 &lt;a href=&quot;http://wiki.ros.org/Books/Programming_Robots_with_ROS&quot;&gt;这本书&lt;/a&gt;，里面前面有很清楚的例子，也是围绕indigo这个版本来教学的，资料也非常多，做了一些简单的小bot。例如wander_bot, follow_bot等等。&lt;/p&gt;

&lt;p&gt;但是对于坐标变换、图像处理、路径规划、地图导航等几个之后常用的方面还是了解甚少，所以特意把这几块的学习部分记录下来。&lt;/p&gt;

&lt;h2 id=&quot;setting-up-robot-using-tf&quot;&gt;Setting up robot using tf&lt;/h2&gt;
&lt;p&gt;这里主要参考ros wiki上的 &lt;a href=&quot;http://wiki.ros.org/navigation/Tutorials/RobotSetup/TF&quot;&gt;这篇文章&lt;/a&gt;。&lt;/p&gt;

&lt;h3 id=&quot;basic-concepts&quot;&gt;Basic Concepts&lt;/h3&gt;
&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;TF&lt;/code&gt;主要用于有很多不同的frame，而各自处理坐标变换极为复杂的情况。对于如下的简单例子，&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://wiki.ros.org/navigation/Tutorials/RobotSetup/TF?action=AttachFile&amp;amp;do=get&amp;amp;target=simple_robot.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;有两个frame，&lt;code class=&quot;highlighter-rouge&quot;&gt;base_laser&lt;/code&gt; 和 &lt;code class=&quot;highlighter-rouge&quot;&gt;base_link&lt;/code&gt;，&lt;code class=&quot;highlighter-rouge&quot;&gt;TF&lt;/code&gt;用一个树结构来保证两个frame间的单向连接。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://wiki.ros.org/navigation/Tutorials/RobotSetup/TF?action=AttachFile&amp;amp;do=get&amp;amp;target=tf_robot.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;首先要确定何为父节点何为儿子节点，再确定连接的方向。&lt;/p&gt;

&lt;h3 id=&quot;writing-codes&quot;&gt;Writing codes&lt;/h3&gt;
&lt;p&gt;以&lt;code class=&quot;highlighter-rouge&quot;&gt;base_link&lt;/code&gt;为父节点，我们将&lt;code class=&quot;highlighter-rouge&quot;&gt;base_laser&lt;/code&gt;的数据转换到&lt;code class=&quot;highlighter-rouge&quot;&gt;base_link&lt;/code&gt;上。首先创建一个package，使用 &lt;code class=&quot;highlighter-rouge&quot;&gt;roscpp&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;tf&lt;/code&gt; 和 &lt;code class=&quot;highlighter-rouge&quot;&gt;geometry_msgs&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;err&quot;&gt;$&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cd&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TOP_DIR_YOUR_CATKIN_WS&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;src&lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;$&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;catkin_create_pkg&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;robot_setup_tf&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;roscpp&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;geometry_msgs&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;broadcasting-a-transform&quot;&gt;Broadcasting a Transform&lt;/h3&gt;
&lt;p&gt;接下来要创建一个node把 &lt;code class=&quot;highlighter-rouge&quot;&gt;base_laser → base_link&lt;/code&gt; 的变换广播给ROS。&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;cp&quot;&gt;#include &amp;lt;ros/ros.h&amp;gt;
#include &amp;lt;tf/transform_broadcaster.h&amp;gt;
&lt;/span&gt;
&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;argc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;argv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;ros&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;init&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;argc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;argv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;robot_tf_publisher&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;ros&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;NodeHandle&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

  &lt;span class=&quot;n&quot;&gt;ros&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Rate&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

  &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TransformBroadcaster&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;broadcaster&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

  &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ok&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()){&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;broadcaster&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sendTransform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;StampedTransform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Quaternion&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Vector3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;ros&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Time&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;now&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;base_link&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;base_laser&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sleep&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The tf package provides an implementation of a &lt;strong&gt;tf::TransformBroadcaster&lt;/strong&gt; to help make the task of publishing transforms easier. To use the TransformBroadcaster, we need to include the tf/transform_broadcaster.h header file.&lt;/p&gt;

&lt;p&gt;Sending a transform with a &lt;strong&gt;TransformBroadcaster requires five arguments&lt;/strong&gt;. First, we pass in the rotation transform, which is specified by a btQuaternion for any rotation that needs to occur between the two coordinate frames. In this case, we want to apply no rotation, so we send in a btQuaternion constructed from pitch, roll, and yaw values equal to zero. Second, a btVector3 for any translation that we’d like to apply. We do, however, want to apply a translation, so we create a btVector3 corresponding to the laser’s x offset of 10cm and z offset of 20cm from the robot base. Third, we need to give the transform being published a timestamp, we’ll just stamp it with ros::Time::now(). Fourth, we need to pass the name of the parent node of the link we’re creating, in this case “base_link.” Fifth, we need to pass the name of the child node of the link we’re creating, in this case “base_laser.”&lt;/p&gt;

&lt;h3 id=&quot;using-a-transform&quot;&gt;Using a Transform&lt;/h3&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;cp&quot;&gt;#include &amp;lt;ros/ros.h&amp;gt;
#include &amp;lt;geometry_msgs/PointStamped.h&amp;gt;
#include &amp;lt;tf/transform_listener.h&amp;gt;
&lt;/span&gt;
&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;transformPoint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TransformListener&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;listener&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;//we'll create a point in the base_laser frame that we'd like to transform to the base_link frame
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;geometry_msgs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;PointStamped&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;laser_point&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;laser_point&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;header&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;frame_id&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;base_laser&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

  &lt;span class=&quot;c1&quot;&gt;//we'll just use the most recent transform available for our simple example
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;laser_point&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;header&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stamp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ros&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;

  &lt;span class=&quot;c1&quot;&gt;//just an arbitrary point in space
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;laser_point&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;point&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;laser_point&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;point&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;laser_point&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;point&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

  &lt;span class=&quot;k&quot;&gt;try&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;geometry_msgs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;PointStamped&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;base_point&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;listener&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transformPoint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;base_link&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;laser_point&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;base_point&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;ROS_INFO&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;base_laser: (%.2f, %.2f. %.2f) -----&amp;gt; base_link: (%.2f, %.2f, %.2f) at time %.2f&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;laser_point&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;point&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;laser_point&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;point&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;laser_point&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;point&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;base_point&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;point&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;base_point&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;point&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;base_point&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;point&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;base_point&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;header&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stamp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;toSec&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;());&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;catch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TransformException&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ex&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ROS_ERROR&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Received an exception trying to transform a point from &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;base_laser&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; to &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;base_link&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;: %s&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ex&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;what&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;());&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;argc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;argv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;ros&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;init&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;argc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;argv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;robot_tf_listener&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;ros&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;NodeHandle&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

  &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TransformListener&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;listener&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ros&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Duration&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;

  &lt;span class=&quot;c1&quot;&gt;//we'll transform a point once every second
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;ros&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Timer&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;timer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;createTimer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ros&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Duration&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;boost&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bind&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transformPoint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;boost&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ref&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;listener&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)));&lt;/span&gt;

  &lt;span class=&quot;n&quot;&gt;ros&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;spin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;

&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We’ll create a function that, given a TransformListener, takes a point in the “base_laser” frame and transforms it to the “base_link” frame. This function will serve as a callback for the ros::Timer created in the main() of our program and will fire every second.&lt;/p&gt;

&lt;p&gt;Here, we’ll create our point as a &lt;code class=&quot;highlighter-rouge&quot;&gt;geometry_msgs::PointStamped&lt;/code&gt;. The “Stamped” on the end of the message name just means that it includes a header, allowing us to associate both a timestamp and a frame_id with the message. We’ll set the stamp field of the laser_point message to be ros::Time() which is a special time value that allows us to ask the TransformListener for the latest available transform. As for the frame_id field of the header, we’ll set that to be “base_laser” because we’re creating a point in the “base_laser” frame.&lt;/p&gt;

&lt;p&gt;Now that we have the point in the “base_laser” frame we want to transform it into the “base_link” frame. To do this, we’ll use the TransformListener object, and call transformPoint() with three arguments: the name of the frame we want to transform the point to (“base_link” in our case), the point we’re transforming, and storage for the transformed point. So, after the call to transformPoint(), base_point holds the same information as laser_point did before only now in the “base_link” frame.&lt;/p&gt;

&lt;h3 id=&quot;build-code&quot;&gt;Build code&lt;/h3&gt;
&lt;p&gt;Now that we’ve written our little example, we need to build it. Open up the &lt;code class=&quot;highlighter-rouge&quot;&gt;CMakeLists.txt&lt;/code&gt; file that is autogenerated by roscreate-pkg and add the following lines to the bottom of the file.&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;add_executable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf_broadcaster&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;src&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf_broadcaster&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cpp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;add_executable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf_listener&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;src&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf_listener&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cpp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;target_link_libraries&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf_broadcaster&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;catkin_LIBRARIES&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;target_link_libraries&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf_listener&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;catkin_LIBRARIES&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>Jim Chen</name></author><summary type="html">这次不想从头开始写ROS学习的部分，基础的很多概念都是学习的 这本书，里面前面有很清楚的例子，也是围绕indigo这个版本来教学的，资料也非常多，做了一些简单的小bot。例如wander_bot, follow_bot等等。</summary></entry><entry><title type="html">解决Caffe make runtest失败的方法</title><link href="http://localhost:4000/posts/2017-07-05-%E8%A7%A3%E5%86%B3Caffe-make-runtest%E5%A4%B1%E8%B4%A5%E7%9A%84%E6%96%B9%E6%B3%95" rel="alternate" type="text/html" title="解决Caffe make runtest失败的方法" /><published>2017-07-05T10:30:00+08:00</published><updated>2017-07-05T10:30:00+08:00</updated><id>http://localhost:4000/posts/%E8%A7%A3%E5%86%B3Caffe-make-runtest%E5%A4%B1%E8%B4%A5%E7%9A%84%E6%96%B9%E6%B3%95</id><content type="html" xml:base="http://localhost:4000/posts/2017-07-05-%E8%A7%A3%E5%86%B3Caffe-make-runtest%E5%A4%B1%E8%B4%A5%E7%9A%84%E6%96%B9%E6%B3%95">&lt;p&gt;在实验室的服务器上执行make runtest时会出现如下的错误：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[Failed] SGDSolverTest/2.TestLeastSquaresUpdate, where TypeParam = caffe::GPUDevices&amp;lt;float&amp;gt;
...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;这里的报错信息里面显示的是“GPUDevice”而不是“CPUDevice”，这种情况可能是因为配置了&lt;strong&gt;多显卡环境&lt;/strong&gt;，可以使用安装CUDA时运行./deviceQuery时标注的0号GPU(“Device 0”)跑跑测试试试看。使用如下命令：&lt;code class=&quot;highlighter-rouge&quot;&gt;export CUDA_VISIBLE_DEVICES=0&lt;/code&gt;，然后重新 &lt;code class=&quot;highlighter-rouge&quot;&gt;make runtest&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;本文参考自：&lt;a href=&quot;http://blog.csdn.net/wonengguwozai/article/details/52724409&quot;&gt;编译caffe源码时,make runtest问题解决方案&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;在此顺带附上实验环境和Makefile.config文件内容&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;系统：Ubuntu 16.04&lt;/li&gt;
  &lt;li&gt;CUDA：8.0&lt;/li&gt;
  &lt;li&gt;cuDNN：5.1.10&lt;/li&gt;
  &lt;li&gt;Caffe：Caffe 1 Latest version&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Makefile.config&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;## Refer to http://caffe.berkeleyvision.org/installation.html
# Contributions simplifying and improving our build system are welcome!

# cuDNN acceleration switch (uncomment to build with cuDNN).
USE_CUDNN := 1
# CUDNN_PATH := /home/libs/cuda-5.0

# CPU-only switch (uncomment to build without GPU support).
# CPU_ONLY := 1

# uncomment to disable IO dependencies and corresponding data layers
# USE_OPENCV := 0
# USE_LEVELDB := 0
# USE_LMDB := 0

# uncomment to allow MDB_NOLOCK when reading LMDB files (only if necessary)
#	You should not set this flag if you will be reading LMDBs with any
#	possibility of simultaneous read and write
# ALLOW_LMDB_NOLOCK := 1

# Uncomment if you're using OpenCV 3
# OPENCV_VERSION := 3

# To customize your choice of compiler, uncomment and set the following.
# N.B. the default for Linux is g++ and the default for OSX is clang++
# CUSTOM_CXX := g++

# CUDA directory contains bin/ and lib/ directories that we need.
CUDA_DIR := /usr/local/cuda
# On Ubuntu 14.04, if cuda tools are installed via
# &quot;sudo apt-get install nvidia-cuda-toolkit&quot; then use this instead:
# CUDA_DIR := /usr

# CUDA architecture setting: going with all of them.
# For CUDA &amp;lt; 6.0, comment the *_50 lines for compatibility.
CUDA_ARCH := -gencode arch=compute_20,code=sm_20 \
		-gencode arch=compute_20,code=sm_21 \
		-gencode arch=compute_30,code=sm_30 \
		-gencode arch=compute_35,code=sm_35 \
		-gencode arch=compute_50,code=sm_50 \
		-gencode arch=compute_50,code=compute_50

# BLAS choice:
# atlas for ATLAS (default)
# mkl for MKL
# open for OpenBlas
BLAS := atlas
# Custom (MKL/ATLAS/OpenBLAS) include and lib directories.
# Leave commented to accept the defaults for your choice of BLAS
# (which should work)!
# BLAS_INCLUDE := /opt/OpenBLAS/include
# BLAS_LIB := /opt/OpenBLAS/lib

# Homebrew puts openblas in a directory that is not on the standard search path
# BLAS_INCLUDE := $(shell brew --prefix openblas)/include
# BLAS_LIB := $(shell brew --prefix openblas)/lib

# This is required only if you will compile the matlab interface.
# MATLAB directory should contain the mex binary in /bin.
# MATLAB_DIR := /usr/local
# MATLAB_DIR := /Applications/MATLAB_R2012b.app

# NOTE: this is required only if you will compile the python interface.
# We need to be able to find Python.h and numpy/arrayobject.h.
PYTHON_INCLUDE := /usr/include/python2.7 \
		/usr/lib/python2.7/dist-packages/numpy/core/include
# Anaconda Python distribution is quite popular. Include path:
# Verify anaconda location, sometimes it's in root.
# ANACONDA_HOME := $(HOME)/anaconda
# PYTHON_INCLUDE := $(ANACONDA_HOME)/include \
		# $(ANACONDA_HOME)/include/python2.7 \
		# $(ANACONDA_HOME)/lib/python2.7/site-packages/numpy/core/include \

# Uncomment to use Python 3 (default is Python 2)
# PYTHON_LIBRARIES := boost_python3 python3.5m
# PYTHON_INCLUDE := /usr/include/python3.5m \
#                 /usr/lib/python3.5/dist-packages/numpy/core/include

# We need to be able to find libpythonX.X.so or .dylib.
PYTHON_LIB := /usr/lib
# PYTHON_LIB := $(ANACONDA_HOME)/lib

# Homebrew installs numpy in a non standard path (keg only)
# PYTHON_INCLUDE += $(dir $(shell python -c 'import numpy.core; print(numpy.core.__file__)'))/include
# PYTHON_LIB += $(shell brew --prefix numpy)/lib

# Uncomment to support layers written in Python (will link against Python libs)
# WITH_PYTHON_LAYER := 1

# Whatever else you find you need goes here.
INCLUDE_DIRS := $(PYTHON_INCLUDE) /usr/local/include /usr/include/hdf5/serial/ # $(CUDNN_PATH)/include
LIBRARY_DIRS := $(PYTHON_LIB) /usr/local/lib /usr/lib /usr/lib/x86_64-linux-gnu /usr/lib/x86_64-linux-gnu/hdf5/serial # $(CUDNN_PATH)/lib64

# If Homebrew is installed at a non standard location (for example your home directory) and you use it for general dependencies
# INCLUDE_DIRS += $(shell brew --prefix)/include
# LIBRARY_DIRS += $(shell brew --prefix)/lib

# Uncomment to use `pkg-config` to specify OpenCV library paths.
# (Usually not necessary -- OpenCV libraries are normally installed in one of the above $LIBRARY_DIRS.)
# USE_PKG_CONFIG := 1

BUILD_DIR := build
DISTRIBUTE_DIR := distribute

# Uncomment for debugging. Does not work on OSX due to https://github.com/BVLC/caffe/issues/171
# DEBUG := 1

# The ID of the GPU that 'make runtest' will use to run unit tests.
TEST_GPUID := 0

# enable pretty build (comment to see full commands)
Q ?= @
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>Jim Chen</name></author><summary type="html">在实验室的服务器上执行make runtest时会出现如下的错误：</summary></entry><entry><title type="html">Learn DAN prototxt</title><link href="http://localhost:4000/posts/2017-07-05-Learn-DAN-prototxt" rel="alternate" type="text/html" title="Learn DAN prototxt" /><published>2017-07-05T10:30:00+08:00</published><updated>2017-07-05T10:30:00+08:00</updated><id>http://localhost:4000/posts/Learn-DAN-prototxt</id><content type="html" xml:base="http://localhost:4000/posts/2017-07-05-Learn-DAN-prototxt">&lt;h2 id=&quot;about-dan&quot;&gt;About DAN&lt;/h2&gt;
&lt;p&gt;DAN (Deep Adaptation Network) was proposed to learn transferable features which generalize well to novel tasks for &lt;code class=&quot;highlighter-rouge&quot;&gt;Domain Adaptation&lt;/code&gt;. In DAN, hidden representa- tions of all task-specific layers are embedded in a reproducing kernel Hilbert space where the mean embeddings of different domain distributions can be explicitly matched. The domain discrepancy is further reduced using an optimal multi-kernel selection method for mean embedding matching.[1]&lt;/p&gt;

&lt;p&gt;Here is the main structure of DAN.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/DAN_architecture.png&quot; alt=&quot;DAN Architecture&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Since deep features eventually transition from general to specific along the network, (1) the features extracted by convolutional layers &lt;code class=&quot;highlighter-rouge&quot;&gt;conv1&lt;/code&gt;–&lt;code class=&quot;highlighter-rouge&quot;&gt;conv3&lt;/code&gt; are general, hence these layers are frozen, (2) the features extracted by layers &lt;code class=&quot;highlighter-rouge&quot;&gt;conv4&lt;/code&gt;–&lt;code class=&quot;highlighter-rouge&quot;&gt;conv5&lt;/code&gt; are slightly less transferable, hence these layers are learned via fine-tuning, and (3) fully connected layers &lt;code class=&quot;highlighter-rouge&quot;&gt;fc6&lt;/code&gt;–&lt;code class=&quot;highlighter-rouge&quot;&gt;fc8&lt;/code&gt; are tailored to fit specific tasks, hence they are not transferable and should be adapted with MK-MMD.&lt;/p&gt;

&lt;p&gt;Following we will detail the &lt;code class=&quot;highlighter-rouge&quot;&gt;*.prototxt&lt;/code&gt; file of DAN.&lt;/p&gt;

&lt;h2 id=&quot;train_valprototxt&quot;&gt;train_val.prototxt&lt;/h2&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;name: &quot;amazon_to_webcam&quot;
layer {           # source domain data layer
  name: &quot;source_data&quot;
  type: &quot;ImageData&quot;          # 直接使用 Raw Image 作为输入
  top: &quot;source_data&quot;
  top: &quot;lp_labels&quot;
  image_data_param {
    source: &quot;./data/office/amazon_list.txt&quot;
    batch_size: 64
    shuffle: true
    new_height: 256
    new_width: 256
  }
  transform_param {          # 进行数据的转换
    crop_size: 227
    # 如果我们输入的图片尺寸大于crop_size，那么图片会被裁剪。
    # 当 phase 模式为 TRAIN 时，裁剪是随机进行裁剪
    # 而当为TEST 模式时，其裁剪方式则只是裁剪图像的中间区域。
    mean_file: &quot;./data/ilsvrc12/imagenet_mean.binaryproto&quot;。    
    # 图片减去均值再训练，会提高训练速度和精度。因此，一般都会提供数据集的均值文件。
    mirror: true         # mirror可以产生镜像,弥补小数据集的不足
  }
  include: { phase: TRAIN }
}
layer {            # target domain data layer
  name: &quot;target_data&quot;
  type: &quot;ImageData&quot;
  top: &quot;target_data&quot;
  top: &quot;target_label&quot;
  image_data_param {
    source: &quot;./data/office/webcam_list.txt&quot;
    batch_size: 64
    shuffle: true
    new_height: 256
    new_width: 256
  }
  transform_param {
    crop_size: 227
    mean_file: &quot;./data/ilsvrc12/imagenet_mean.binaryproto&quot;
    mirror: true
  }
  include: { phase: TRAIN }
}
layer {            # silence layeris simply to avoid that the output of unused blobs is reported in the log. 
  name: &quot;target_label_silence&quot;
  type: &quot;Silence&quot;
  bottom: &quot;target_label&quot;
  include: { phase: TRAIN}
}
layer {            # target data layer for test. So `mirror` is set to false.
  name: &quot;target_data&quot;
  type: &quot;ImageData&quot;
  top: &quot;data&quot;
  top: &quot;lp_labels&quot;
  image_data_param {
    source: &quot;./data/office/webcam_list.txt&quot;
    batch_size: 1
    shuffle: true
    new_height: 256
    new_width: 256
  }
  transform_param {
    crop_size: 227
    mean_file: &quot;./data/ilsvrc12/imagenet_mean.binaryproto&quot;
    mirror: false
  }
  include: { phase: TEST }
}

# ---------------------------------------------------------- source and target data concatenation

layer {
  name: &quot;concat_data&quot;
  type: &quot;Concat&quot;
  bottom: &quot;source_data&quot;
  bottom: &quot;target_data&quot;
  top: &quot;data&quot;
  concat_param {
    concat_dim: 0
  }
  include: { phase: TRAIN }
}

# ---------------------------------------------------------- convolution

### layer {          # output h_o = (h_i + 2 * pad_h - kernel_h) / stride_h + 1
  name: &quot;conv1&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;data&quot;
  top: &quot;conv1&quot;
  param {       # learning rate and decay multipliers for the filters
    lr_mult: 1
    decay_mult: 1
  }
  param {       # learning rate and decay multipliers for the biases
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96          # number of filters
    kernel_size: 11         # kernel size
    stride: 4         
    weight_filler {     # parameter for weight initialization   
      type: &quot;gaussian&quot;  # initialize the filters from a Gaussian
      std: 0.01         # distribution with stdev 0.01 (default mean: 0)
    }
    bias_filler {       # parameter for bias initialization
      type: &quot;constant&quot;
      value: 0
    }
  }
}
layer {
  name: &quot;relu1&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;conv1&quot;
  top: &quot;conv1&quot;
}
layer {            # 局部响应归一化层完成一种“临近抑制”操作，对局部输入区域进行归一化。
  name: &quot;norm1&quot;
  type: &quot;LRN&quot;
  bottom: &quot;conv1&quot;
  top: &quot;norm1&quot;
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: &quot;pool1&quot;
  type: &quot;Pooling&quot;
  bottom: &quot;norm1&quot;
  top: &quot;pool1&quot;
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: &quot;conv2&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;pool1&quot;
  top: &quot;conv2&quot;
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.01
    }
    bias_filler {
      type: &quot;constant&quot;
      value: 0.1
    }
  }
}
layer {
  name: &quot;relu2&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;conv2&quot;
  top: &quot;conv2&quot;
}
layer {
  name: &quot;norm2&quot;
  type: &quot;LRN&quot;
  bottom: &quot;conv2&quot;
  top: &quot;norm2&quot;
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: &quot;pool2&quot;
  type: &quot;Pooling&quot;
  bottom: &quot;norm2&quot;
  top: &quot;pool2&quot;
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: &quot;conv3&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;pool2&quot;
  top: &quot;conv3&quot;
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.01
    }
    bias_filler {
      type: &quot;constant&quot;
      value: 0
    }
  }
}
layer {
  name: &quot;relu3&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;conv3&quot;
  top: &quot;conv3&quot;
}
layer {
  name: &quot;conv4&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;conv3&quot;
  top: &quot;conv4&quot;
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.01
    }
    bias_filler {
      type: &quot;constant&quot;
      value: 0.1
    }
  }
}
layer {
  name: &quot;relu4&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;conv4&quot;
  top: &quot;conv4&quot;
}
layer {
  name: &quot;conv5&quot;
  type: &quot;Convolution&quot;
  bottom: &quot;conv4&quot;
  top: &quot;conv5&quot;
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.01
    }
    bias_filler {
      type: &quot;constant&quot;
      value: 0.1
    }
  }
}
layer {
  name: &quot;relu5&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;conv5&quot;
  top: &quot;conv5&quot;
}
layer {
  name: &quot;pool5&quot;
  type: &quot;Pooling&quot;
  bottom: &quot;conv5&quot;
  top: &quot;pool5&quot;
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: &quot;fc6&quot;
  type: &quot;InnerProduct&quot;
  bottom: &quot;pool5&quot;
  top: &quot;fc6&quot; 
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.005
    }
    bias_filler {
      type: &quot;constant&quot;
      value: 0.1
    }
  }
}
layer {
  name: &quot;relu6&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;fc6&quot;
  top: &quot;fc6&quot;
}
layer {           # Dropout层在激活之后
  name: &quot;drop6&quot;
  type: &quot;Dropout&quot;
  bottom: &quot;fc6&quot;
  top: &quot;fc6&quot;
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: &quot;fc7&quot;
  type: &quot;InnerProduct&quot;
  bottom: &quot;fc6&quot;
  top: &quot;fc7&quot;
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.005
    }
    bias_filler {
      type: &quot;constant&quot;
      value: 0.1
    }
  }
}
layer {
  name: &quot;relu7&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;fc7&quot;
  top: &quot;fc7&quot;
}
layer {
  name: &quot;drop7&quot;
  type: &quot;Dropout&quot;
  bottom: &quot;fc7&quot;
  top: &quot;fc7&quot;
  dropout_param {
    dropout_ratio: 0.5
  }
}

# ---------------------------------------------------------- alias fc7 to source_features_fc7 in test

layer {     # The Split layer is a utility layer that splits an input blob to multiple output blobs. This is used when a blob is fed into multiple output layers.
  name: &quot;fc7_alias&quot;
  type: &quot;Split&quot;
  bottom: &quot;fc7&quot;
  top: &quot;source_features_fc7&quot;
  include: { phase: TEST }
}

# ---------------------------------------------------------- split source and target in train

layer {     # 与split layer的不一样在于split的作用是将bottom复制多份，输出到tops.
  name: &quot;slice_features_fc7&quot;
  type: &quot;Slice&quot;
  bottom: &quot;fc7&quot;
  top: &quot;source_features_fc7&quot;
  top: &quot;target_features_fc7&quot;
  slice_param {
    slice_dim: 0
  }
  include: { phase: TRAIN }
}

# ---------------------------------------------------------- fc8 of source

layer {
  name: &quot;fc8_source&quot;
  type: &quot;InnerProduct&quot;
  bottom: &quot;source_features_fc7&quot;
  top: &quot;source_features_fc8&quot;
  param {
    name: &quot;fc8_w&quot;
    lr_mult: 10
    decay_mult: 1
  }
  param {
    name: &quot;fc8_b&quot;
    lr_mult: 20
    decay_mult: 0
  }
  inner_product_param {
    num_output: 31
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.01
    }
    bias_filler {
      type: &quot;constant&quot;
      value: 0
    }
  }
}

layer {
  name: &quot;accuracy&quot;
  type: &quot;Accuracy&quot;
  bottom: &quot;source_features_fc8&quot;
  bottom: &quot;lp_labels&quot;
  top: &quot;lp_accuracy&quot;
  include: { phase: TEST }
}

layer {
  name: &quot;softmax_loss&quot;
  type: &quot;SoftmaxWithLoss&quot;
  bottom: &quot;source_features_fc8&quot;
  bottom: &quot;lp_labels&quot;
  top: &quot;softmax_loss&quot;
  include: { phase: TRAIN }
}

# ---------------------------------------------------------- fc8 of target

layer {
  name: &quot;fc8_target&quot;
  type: &quot;InnerProduct&quot;
  bottom: &quot;target_features_fc7&quot;
  top: &quot;target_features_fc8&quot;
  param {
    name: &quot;fc8_w&quot;
    lr_mult: 10
    decay_mult: 1
  }
  param {
    name: &quot;fc8_b&quot;
    lr_mult: 20
    decay_mult: 0
  }
  inner_product_param {
    num_output: 31
    weight_filler {
      type: &quot;gaussian&quot;
      std: 0.01
    }
    bias_filler {
      type: &quot;constant&quot;
      value: 0
    }
  }
  include: { phase: TRAIN }
}

# ---------------------------------------------------------- mmd of fc7 and fc8

layer {
  name: &quot;mmd_loss_fc7&quot;
  type: &quot;MMDLoss&quot;
  bottom: &quot;source_features_fc7&quot;
  bottom: &quot;target_features_fc7&quot;
  top: &quot;fc7_mmd_loss&quot;
  loss_weight: 1
  mmd_param {
    kernel_num: 5
    kernel_mul: 2.0
    fix_gamma: false
  }
  include: { phase: TRAIN }
}

layer {
  name: &quot;mmd_loss_fc8&quot;
  type: &quot;MMDLoss&quot;
  bottom: &quot;source_features_fc8&quot;
  bottom: &quot;target_features_fc8&quot;
  top: &quot;fc8_mmd_loss&quot;
  loss_weight: 1
  mmd_param {
    kernel_num: 5
    kernel_mul: 2.0
    fix_gamma: false
  }
  include: { phase: TRAIN }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;solverprototxt&quot;&gt;solver.prototxt&lt;/h2&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;net: &quot;./models/DAN/amazon_to_webcam/train_val.prototxt&quot;
test_iter: 795    
test_interval: 300    # 测试间隔。也就是每训练300次，才进行一次测试。
base_lr: 0.0003       # 基础学习率
lr_policy: &quot;inv&quot;      # 学习率调整的策略
gamma: 0.002
power: 0.75
momentum: 0.9         # 上一次梯度更新的权重,用来加权之前梯度方向对现在梯度下降方向的影响.
display: 100
max_iter: 50000
snapshot: 60000
snapshot_prefix: &quot;./models/DAN/amazon_to_webcam/trained_model&quot;
solver_mode: GPU
snapshot_after_train: false
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;test_iter:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;这个要与test layer中的batch_size结合起来理解。mnist数据中测试样本总数为10000，一次性执行全部数据效率很低，因此我们将测试数据分成几个批次来执行，每个批次的数量就是batch_size。假设我们设置batch_size为100，则需要迭代100次才能将10000个数据全部执行完。因此test_iter设置为100。在这里，batch_size=1，所以test_iter是总数795。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;lr_policy:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;fixed:　　 保持base_lr不变.&lt;/li&gt;
  &lt;li&gt;step: 　　 如果设置为step,则还需要设置一个stepsize,  返回 &lt;code class=&quot;highlighter-rouge&quot;&gt;base_lr*gamma^(floor(iter/stepsize))&lt;/code&gt;,其中&lt;code class=&quot;highlighter-rouge&quot;&gt;iter&lt;/code&gt;表示当前的迭代次数&lt;/li&gt;
  &lt;li&gt;exp:   　　返回&lt;code class=&quot;highlighter-rouge&quot;&gt;base_lr * gamma ^ iter&lt;/code&gt;， &lt;code class=&quot;highlighter-rouge&quot;&gt;iter&lt;/code&gt;为当前迭代次数&lt;/li&gt;
  &lt;li&gt;inv:　　   如果设置为inv,还需要设置一个power, 返回&lt;code class=&quot;highlighter-rouge&quot;&gt;base_lr * (1 + gamma * iter) ^ (- power)&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;multistep: 如果设置为multistep,则还需要设置一个stepvalue。这个参数和step很相似，step是均匀等间隔变化，而multistep则是根据stepvalue值变化&lt;/li&gt;
  &lt;li&gt;poly: 　　  学习率进行多项式误差, 返回 &lt;code class=&quot;highlighter-rouge&quot;&gt;base_lr (1 - iter/max_iter) ^ (power)&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;sigmoid:　 学习率进行sigmod衰减，返回 &lt;code class=&quot;highlighter-rouge&quot;&gt;base_lr ( 1/(1 + exp(-gamma * (iter - stepsize))))&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;refenrence&quot;&gt;Refenrence&lt;/h2&gt;
&lt;p&gt;[1] Long M, Cao Y, Wang J, et al. Learning transferable features with deep adaptation networks[C]//International Conference on Machine Learning. 2015: 97-105.&lt;/p&gt;</content><author><name>Jim Chen</name></author><summary type="html">About DAN DAN (Deep Adaptation Network) was proposed to learn transferable features which generalize well to novel tasks for Domain Adaptation. In DAN, hidden representa- tions of all task-specific layers are embedded in a reproducing kernel Hilbert space where the mean embeddings of different domain distributions can be explicitly matched. The domain discrepancy is further reduced using an optimal multi-kernel selection method for mean embedding matching.[1]</summary></entry><entry><title type="html">Caffe Learning Notes(4)</title><link href="http://localhost:4000/posts/2017-07-04-Caffe-Learning-Notes(4)" rel="alternate" type="text/html" title="Caffe Learning Notes(4)" /><published>2017-07-04T20:00:00+08:00</published><updated>2017-07-04T20:00:00+08:00</updated><id>http://localhost:4000/posts/Caffe-Learning-Notes(4)</id><content type="html" xml:base="http://localhost:4000/posts/2017-07-04-Caffe-Learning-Notes(4)">&lt;h1 id=&quot;模块学习layer&quot;&gt;模块学习——Layer&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;/images/layer_structure.png&quot; alt=&quot;Layer structures&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Activation/NeuronLayer&lt;/code&gt;类 定义于&lt;code class=&quot;highlighter-rouge&quot;&gt;neuron_layers.hpp&lt;/code&gt;中，其派生类主要是元素级别的运算（比如Dropout运算，激活函数ReLu，Sigmoid等），运算均为同址计算（in-place computation，返回值覆盖原值而占用新的内存）。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;LossLayer&lt;/code&gt;类 定义于&lt;code class=&quot;highlighter-rouge&quot;&gt;loss_layers.hpp&lt;/code&gt;中，其派生类会产生loss，只有这些层能够产生loss。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;DataLayer&lt;/code&gt; 定义于&lt;code class=&quot;highlighter-rouge&quot;&gt;data_layer.hpp&lt;/code&gt;中，作为网络的最底层，主要实现数据格式的转换。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Vision&lt;/code&gt; Layer 定义于&lt;code class=&quot;highlighter-rouge&quot;&gt;vision_layers.hpp&lt;/code&gt;，实现特征表达功能，更具体地说包含卷积操作，Pooling操作，他们基本都会产生新的内存占用（Pooling相对较小）。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Common Layers&lt;/code&gt; 定义于&lt;code class=&quot;highlighter-rouge&quot;&gt;common_layers.hpp&lt;/code&gt;，Caffe提供了单个层与多个层的连接，并在这个头文件中声明。这里还包括了常用的全连接层&lt;code class=&quot;highlighter-rouge&quot;&gt;InnerProductLayer&lt;/code&gt;类。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;首先来看layer类的构造部分，以及Public部分的函数&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;template &amp;lt;typename Dtype&amp;gt;
class Layer {
 public:
  explicit Layer(const LayerParameter&amp;amp; param)
    : layer_param_(param), is_shared_(false) {
      // Set phase and copy blobs (if there are any).
      phase_ = param.phase();
      if (layer_param_.blobs_size() &amp;gt; 0) {
        blobs_.resize(layer_param_.blobs_size());
        for (int i = 0; i &amp;lt; layer_param_.blobs_size(); ++i) {
          blobs_[i].reset(new Blob&amp;lt;Dtype&amp;gt;());
          blobs_[i]-&amp;gt;FromProto(layer_param_.blobs(i));
        }
      }
    }
  virtual ~Layer() {}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Layer层的主要的参数和成员变量:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; /** The protobuf that stores the layer parameters */
 // 层说明参数，从protocal buffers格式的网络结构说明文件中读取
  LayerParameter layer_param_;
  /** The phase: TRAIN or TEST */
  // 层状态，参与网络的训练还是测试
  Phase phase_;
  /** The vector that stores the learnable parameters as a set of blobs. */
  // 层权值和偏置参数，使用向量是因为权值参数和偏置是分开保存在两个blob中的
  vector&amp;lt;shared_ptr&amp;lt;Blob&amp;lt;Dtype&amp;gt; &amp;gt; &amp;gt; blobs_;
  /** Vector indicating whether to compute the diff of each param blob. */
  // 标志每个top blob是否需要计算反向传递的梯度值
  vector&amp;lt;bool&amp;gt; param_propagate_down_;

  /** The vector that indicates whether each top blob has a non-zero weight in
   *  the objective function. */
   // 每一个top blob中是否有非零的权值
  vector&amp;lt;Dtype&amp;gt; loss_;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;首先获得当前网络的phase，是&lt;code class=&quot;highlighter-rouge&quot;&gt;train&lt;/code&gt;还是&lt;code class=&quot;highlighter-rouge&quot;&gt;test&lt;/code&gt;，在初始化列表初始化&lt;code class=&quot;highlighter-rouge&quot;&gt;LayerParameter&lt;/code&gt;,之后&lt;code class=&quot;highlighter-rouge&quot;&gt;blobs_&lt;/code&gt;这里存放的是一个指向blob类的&lt;code class=&quot;highlighter-rouge&quot;&gt;shared_ptr&lt;/code&gt;指针的一个vector，在这里是申请空间，然后将传入的&lt;code class=&quot;highlighter-rouge&quot;&gt;layer_param&lt;/code&gt;中的blob拷贝过来。&lt;/p&gt;

&lt;p&gt;初始化函数SetUp()&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;/**
   * @brief 实现每个layer对象的setup函数
   * @param bottom 
   * 层的输入数据，blob中的存储空间已申请
   * @param top
   * 层的输出数据，blob对象以构造但是其中的存储空间未申请，
   * 具体空间大小需根据bottom blob大小和layer_param_共同决定，具体在Reshape函数现实

   * 1. 检查输入输出blob个数是否满足要求，每个层能处理的输入输出数据不一样
   * 2. 调用LayerSetUp函数初始化特殊的层，每个Layer子类需重写这个函数完成定制的初始化
   * 3. 调用Reshape函数为top blob分配合适大小的存储空间
   * 4. 为每个top blob设置损失权重乘子，非LossLayer为的top blob其值为零
   *
   * 此方法非虚函数，不用重写，模式固定
   */
  void SetUp(const vector&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt;&amp;amp; bottom,
      const vector&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt;&amp;amp; top) {
    CheckBlobCounts(bottom, top);
    LayerSetUp(bottom, top);
    Reshape(bottom, top);
    SetLossWeights(top);
  }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;这里是&lt;code class=&quot;highlighter-rouge&quot;&gt;Setup&lt;/code&gt;函数，首先check 这个bottom和top的blob是否正确，再调用&lt;code class=&quot;highlighter-rouge&quot;&gt;Layersetup&lt;/code&gt;对每一具体的层做进一步设置，之后再做reshape来设置top blobs和internal buffer。最后再设置loss weight multiplier 的blob对每一个非零的loss和weight，一般这个方法被继承之后是不会被重写的。&lt;/p&gt;

&lt;p&gt;每个子类Layer必须重写的初始化函数LayerSetUp和Reshape函数。&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;LayerSetup&lt;/code&gt;就是对具体某一个layer的setup,被上面的那个函数所调用，&lt;code class=&quot;highlighter-rouge&quot;&gt;ShareInParallel&lt;/code&gt;和&lt;code class=&quot;highlighter-rouge&quot;&gt;IsShared&lt;/code&gt;和&lt;code class=&quot;highlighter-rouge&quot;&gt;SetShared&lt;/code&gt;分别是用来返回并行状态和获得这一layer是否被多个nets所共享，默认是除了data layer都是关闭的。在多个GPU下的Train阶段以及share是true的情况下,is_shared将会被置成true。&lt;/p&gt;

&lt;p&gt;Reshape函数完成top blob形状的设置并为其分配存储空间。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;virtual inline bool ShareInParallel() 
inline bool IsShared() const
inline void SetShared(bool is_shared)

 /**
   * @brief Does layer-specific setup: your layer should implement this function
   *        as well as Reshape.
   *        定制初始化，每个子类layer必须实现此虚函数
   *
   * @param bottom
   *     the preshaped input blobs, whose data fields store the input data for
   *     this layer
   *     输入blob, 数据成员data_和diff_存储了相关数据
   * @param top
   *     the allocated but unshaped output blobs
   *     输出blob, blob对象已构造但数据成员的空间尚未申请
   *
   * This method should do one-time layer specific setup. This includes reading
   * and processing relevent parameters from the &amp;lt;code&amp;gt;layer_param_&amp;lt;/code&amp;gt;.
   * Setting up the shapes of top blobs and internal buffers should be done in
   * &amp;lt;code&amp;gt;Reshape&amp;lt;/code&amp;gt;, which will be called before the forward pass to
   * adjust the top blob sizes.
   * 此方法执行一次定制化的层初始化，包括从layer_param_读入并处理相关的层权值和偏置参数，
   * 调用Reshape函数申请top blob的存储空间
   */
  virtual void LayerSetUp(const vector&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt;&amp;amp; bottom,
      const vector&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt;&amp;amp; top) {}
      
  virtual void Reshape(const vector&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt;&amp;amp; bottom,
      const vector&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt;&amp;amp; top) = 0;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;前向传播函数Forward和反向传播函数Backward。这两个函数非虚函数，它们内部会调用如下虚函数完成数据前向传递和误差反向传播，根据执行环境的不同每个子类Layer必须重写CPU和GPU版本&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  /** @brief Using the CPU device, compute the layer output. */
  virtual void Forward_cpu(const vector&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt;&amp;amp; bottom,
      const vector&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt;&amp;amp; top) = 0;
  /**
   * @brief Using the GPU device, compute the layer output.
   *        Fall back to Forward_cpu() if unavailable.
   */
  virtual void Forward_gpu(const vector&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt;&amp;amp; bottom,
      const vector&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt;&amp;amp; top) {
    // LOG(WARNING) &amp;lt;&amp;lt; &quot;Using CPU code as backup.&quot;;
    return Forward_cpu(bottom, top);
  }

  /**
   * @brief Using the CPU device, compute the gradients for any parameters and
   *        for the bottom blobs if propagate_down is true.
   */
  virtual void Backward_cpu(const vector&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt;&amp;amp; top,
      const vector&amp;lt;bool&amp;gt;&amp;amp; propagate_down,
      const vector&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt;&amp;amp; bottom) = 0;
  /**
   * @brief Using the GPU device, compute the gradients for any parameters and
   *        for the bottom blobs if propagate_down is true.
   *        Fall back to Backward_cpu() if unavailable.
   */
  virtual void Backward_gpu(const vector&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt;&amp;amp; top,
      const vector&amp;lt;bool&amp;gt;&amp;amp; propagate_down,
      const vector&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt;&amp;amp; bottom) {
    // LOG(WARNING) &amp;lt;&amp;lt; &quot;Using CPU code as backup.&quot;;
    Backward_cpu(top, propagate_down, bottom);
  }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;这个函数被setup调用，主要是check bottom和top 的blob是否match，这里面用了上面提到的&lt;code class=&quot;highlighter-rouge&quot;&gt;ExactBottomBlobs()&lt;/code&gt;等函数&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;virtual void CheckBlobCounts(const vector&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt;&amp;amp; bottom, const vector&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt;&amp;amp; top)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;SetLoss&lt;/code&gt;是非常重要的一个步骤，是被&lt;code class=&quot;highlighter-rouge&quot;&gt;SetUp&lt;/code&gt;调用来初始化top bottom的weights，并且存储非零的loss weights 在diff blob里面&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;inline void SetLossWeights(const vector&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt;&amp;amp; top)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;data-layer&quot;&gt;Data Layer&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;data_layer&lt;/code&gt;应该是网络的最底层，主要是将数据送给blob进入到net中，在data_layer中存在多个跟&lt;code class=&quot;highlighter-rouge&quot;&gt;data_layer&lt;/code&gt;相关的类&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;BaseDataLayer&lt;/li&gt;
  &lt;li&gt;BasePrefetchingDataLayer&lt;/li&gt;
  &lt;li&gt;DataLayer&lt;/li&gt;
  &lt;li&gt;DummyDataLayer&lt;/li&gt;
  &lt;li&gt;HDF5DataLayer&lt;/li&gt;
  &lt;li&gt;HDF5OutputLayer&lt;/li&gt;
  &lt;li&gt;ImageDataLayer&lt;/li&gt;
  &lt;li&gt;MemoryDataLayer&lt;/li&gt;
  &lt;li&gt;WindowDataLayer&lt;/li&gt;
  &lt;li&gt;Batch&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;首先Layer是基类，这个之前就已经提到过了。其次看HDF5相关的类有两个，一个是HDF5DataLayer，另一个是HDF5OutputLayer，主要是基于HDF5数据格式的读取和存储&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Data_layer&lt;/code&gt;头文件如下：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;#include &amp;lt;string&amp;gt;
#include &amp;lt;utility&amp;gt;
#include &amp;lt;vector&amp;gt;
#include &quot;hdf5.h&quot;

#include &quot;caffe/blob.hpp&quot;
#include &quot;caffe/common.hpp&quot;
#include &quot;caffe/data_reader.hpp&quot;
#include &quot;caffe/data_transformer.hpp&quot;
#include &quot;caffe/filler.hpp&quot;
#include &quot;caffe/internal_thread.hpp&quot;
#include &quot;caffe/layer.hpp&quot;
#include &quot;caffe/proto/caffe.pb.h&quot;
#include &quot;caffe/util/blocking_queue.hpp&quot;
#include &quot;caffe/util/db.hpp&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;还有几个跟data相关的头文件比如data_read.hpp,data_transformer.hpp
其中data_reader主要是负责数据的读取，传送到data layer中。并且对于每一个source，都会开一一起独立的reading thread读取线程，几十有多个solver在并行的跑。比如在多GPU训练的时候，可以保证对于数据库的读取是顺序的。&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;data_transformer.hpp&lt;/code&gt;里面的&lt;code class=&quot;highlighter-rouge&quot;&gt;DataTransformer&lt;/code&gt;这个类，这个类我们要关注一下，这个类主要能对input data 执一些预处理操作，比如缩放、镜像、减去均值。同时还支持一些随机的操作。&lt;/p&gt;

&lt;p&gt;其核心的函数如下，这里总共有5个常见的Transform函数，其中所有函数的第二部分是相同的，都是一个目标blob，而输入根据输入的情况可以有所选择，可以是blob,也可以是opencv的mat 结构，或者proto中定义的datum结构。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;void Transform(const Datum&amp;amp; datum, Blob&amp;lt;Dtype&amp;gt;* transformed_blob);
void Transform(const vector&amp;lt;Datum&amp;gt; &amp;amp; datum_vector, Blob&amp;lt;Dtype&amp;gt;* transformed_blob);
void Transform(const vector&amp;lt;cv::Mat&amp;gt; &amp;amp; mat_vector, Blob&amp;lt;Dtype&amp;gt;* transformed_blob);
void Transform(const cv::Mat&amp;amp; cv_img, Blob&amp;lt;Dtype&amp;gt;* transformed_blob);
void Transform(Blob&amp;lt;Dtype&amp;gt;* input_blob, Blob&amp;lt;Dtype&amp;gt;* transformed_blob);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;TransformationParameter&lt;/code&gt;是该类构造器中需要传入的一些变形参数，相关的操作定义在proto中，摘录如下,可以看到总共有sacle, mirror, crop_size, mean_file, mean_value, force_color, force_grey 共7个相关操作。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;message TransformationParameter {
  optional float scale = 1 [default = 1];
  optional bool mirror = 2 [default = false];
  optional uint32 crop_size = 3 [default = 0];
  optional string mean_file = 4;
  repeated float mean_value = 5;
  optional bool force_color = 6 [default = false];
  optional bool force_gray = 7 [default = false];
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;最重要的就是类内的&lt;code class=&quot;highlighter-rouge&quot;&gt;layerSetup&lt;/code&gt;.首先我们来看DataLayer的&lt;code class=&quot;highlighter-rouge&quot;&gt;DataLayerSetUp&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;void DataLayer&amp;lt;Dtype&amp;gt;::DataLayerSetUp(const vector&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt;&amp;amp; bottom,
      const vector&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt;&amp;amp; top) {
  const int batch_size = this-&amp;gt;layer_param_.data_param().batch_size();
  //获得相应的datum，用来初始化top blob
  Datum&amp;amp; datum = *(reader_.full().peek());
  //使用data_transformer 来计算根据datum的期望blob的shape
  vector&amp;lt;int&amp;gt; top_shape = this-&amp;gt;data_transformer_-&amp;gt;InferBlobShape(datum);
  this-&amp;gt;transformed_data_.Reshape(top_shape);
  //首先reshape top[0]，再根据batch的大小进行预取
  top_shape[0] = batch_size;
  top[0]-&amp;gt;Reshape(top_shape);
  for (int i = 0; i &amp;lt; this-&amp;gt;PREFETCH_COUNT; ++i) {
    this-&amp;gt;prefetch_[i].data_.Reshape(top_shape);
  }
  LOG(INFO) &amp;lt;&amp;lt; &quot;output data size: &quot; &amp;lt;&amp;lt; top[0]-&amp;gt;num() &amp;lt;&amp;lt; &quot;,&quot;
      &amp;lt;&amp;lt; top[0]-&amp;gt;channels() &amp;lt;&amp;lt; &quot;,&quot; &amp;lt;&amp;lt; top[0]-&amp;gt;height() &amp;lt;&amp;lt; &quot;,&quot;
      &amp;lt;&amp;lt; top[0]-&amp;gt;width();
  // 同样reshape label的blob的shape
  if (this-&amp;gt;output_labels_) {
    vector&amp;lt;int&amp;gt; label_shape(1, batch_size);
    top[1]-&amp;gt;Reshape(label_shape);
    for (int i = 0; i &amp;lt; this-&amp;gt;PREFETCH_COUNT; ++i) {
      this-&amp;gt;prefetch_[i].label_.Reshape(label_shape);
    }
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;还有很多其他的&lt;code class=&quot;highlighter-rouge&quot;&gt;Data_Layer&lt;/code&gt;的初始化，可以参考：&lt;a href=&quot;http://www.cnblogs.com/louyihang-loves-baiyan/p/5153155.html&quot;&gt;Caffe源码解析4：Data_layer&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;conv-layer&quot;&gt;Conv Layer&lt;/h2&gt;
&lt;p&gt;这里先引用一个其他博客里总结的Conv Layer的类关系图&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/conv_layer_structure.jpg&quot; alt=&quot;Conv Layer&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;BaseConvolutionLayer&lt;/code&gt;&lt;/strong&gt;
其继承自Layer，是一个卷积以及反卷积操作的基类，首先我们来看BaseConvolutionLayer的LayerSetUp函数&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;void BaseConvolutionLayer&amp;lt;Dtype&amp;gt;::LayerSetUp(const vector&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt;&amp;amp; bottom,
      const vector&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt;&amp;amp; top)
  //首先这里主要是在配置卷积kernel 的size,padding,stride以及inputs
  ConvolutionParameter conv_param = this-&amp;gt;layer_param_.convolution_param();
  force_nd_im2col_ = conv_param.force_nd_im2col();
  channel_axis_ = bottom[0]-&amp;gt;CanonicalAxisIndex(conv_param.axis());
  const int first_spatial_axis = channel_axis_ + 1;
  const int num_axes = bottom[0]-&amp;gt;num_axes();
  num_spatial_axes_ = num_axes - first_spatial_axis;
  CHECK_GE(num_spatial_axes_, 0);
  vector&amp;lt;int&amp;gt; bottom_dim_blob_shape(1, num_spatial_axes_ + 1);
  vector&amp;lt;int&amp;gt; spatial_dim_blob_shape(1, std::max(num_spatial_axes_, 1));
  // 设置kernel的dimensions
  kernel_shape_.Reshape(spatial_dim_blob_shape);
  int* kernel_shape_data = kernel_shape_.mutable_cpu_data();
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;接着是设置相应的stride dimensions，对于2D，设置在h和w方向上的stride，对于kernel的pad也做相应设置&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pad_.Reshape(spatial_dim_blob_shape);
int* pad_data = pad_.mutable_cpu_data();
pad_data[0] = conv_param.pad_h();
pad_data[1] = conv_param.pad_w();
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;接下来是对&lt;code class=&quot;highlighter-rouge&quot;&gt;widths&lt;/code&gt; 和 &lt;code class=&quot;highlighter-rouge&quot;&gt;bias&lt;/code&gt; 做设置和填充，其中blob[0]里面存放的是filter weights,而blob[1]里面存放的是biases，当然bias是可选的，也可以没有&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;//设置相应的shape，并检查
vector&amp;lt;int&amp;gt; weight_shape(2);
weight_shape[0] = conv_out_channels_;
weight_shape[1] = conv_in_channels_ / group_;

bias_term_ = this-&amp;gt;layer_param_.convolution_param().bias_term();
vector&amp;lt;int&amp;gt; bias_shape(bias_term_, num_output_);

//填充权重
this-&amp;gt;blobs_[0].reset(new Blob&amp;lt;Dtype&amp;gt;(weight_shape));
shared_ptr&amp;lt;Filler&amp;lt;Dtype&amp;gt; &amp;gt; weight_filler(GetFiller&amp;lt;Dtype&amp;gt;(
    this-&amp;gt;layer_param_.convolution_param().weight_filler()));
weight_filler-&amp;gt;Fill(this-&amp;gt;blobs_[0].get());
//填充偏置项
if (bias_term_) {
  this-&amp;gt;blobs_[1].reset(new Blob&amp;lt;Dtype&amp;gt;(bias_shape));
  shared_ptr&amp;lt;Filler&amp;lt;Dtype&amp;gt; &amp;gt; bias_filler(GetFiller&amp;lt;Dtype&amp;gt;(
      this-&amp;gt;layer_param_.convolution_param().bias_filler()));
  bias_filler-&amp;gt;Fill(this-&amp;gt;blobs_[1].get());
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;ConvolutionLayer&lt;/code&gt;&lt;/strong&gt; 继承了&lt;code class=&quot;highlighter-rouge&quot;&gt;BaseConvolutionLayer&lt;/code&gt;，主要作用就是将一副image做卷积操作，使用学到的filter的参数和biaes。同时在Caffe里面，卷积操作做了优化，变成了一个矩阵相乘的操作。其中有两个比较主要的函数是&lt;code class=&quot;highlighter-rouge&quot;&gt;im2col&lt;/code&gt;以及&lt;code class=&quot;highlighter-rouge&quot;&gt;col2im&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;图中上半部分是一个传统卷积，下图是一个矩阵相乘的版本。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/two_conv_type.png&quot; alt=&quot;Two Types of Conv&quot; /&gt;&lt;/p&gt;

&lt;p&gt;下图是在一个卷积层中将卷积操作展开的具体操作过程，他里面按照卷积核的大小取数据然后展开，在同一张图里的不同卷积核选取的逐行摆放，不同N的话，就在同一行后面继续拼接，不同个可以是多个通道，但是需要注意的是同一行里面每一段都应该对应的是原图中中一个位置的卷积窗口。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/conv_cal.png&quot; alt=&quot;Calculate Conv&quot; /&gt;&lt;/p&gt;

&lt;p&gt;对于卷积层中的卷积操作，还有一个group的概念要说明一下，groups是代表filter 组的个数。引入gruop主要是为了选择性的连接卷积层的输入端和输出端的channels，否则参数会太多。每一个group 和1/group的input 通道和 1/group 的output通道进行卷积操作。比如有4个input， 8个output，那么1-4属于第一组，5-8属于第二个gruop。&lt;/p&gt;

&lt;p&gt;ConvolutionLayer里面，主要重写了Forward_cpu和Backward_cpu&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;void ConvolutionLayer&amp;lt;Dtype&amp;gt;::Forward_cpu(const vector&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt;&amp;amp; bottom,
      const vector&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt;&amp;amp; top) {
  const Dtype* weight = this-&amp;gt;blobs_[0]-&amp;gt;cpu_data();
  for (int i = 0; i &amp;lt; bottom.size(); ++i) {
    const Dtype* bottom_data = bottom[i]-&amp;gt;cpu_data();
    Dtype* top_data = top[i]-&amp;gt;mutable_cpu_data();
    for (int n = 0; n &amp;lt; this-&amp;gt;num_; ++n) {
      this-&amp;gt;forward_cpu_gemm(bottom_data + n * this-&amp;gt;bottom_dim_, weight,
          top_data + n * this-&amp;gt;top_dim_);
      if (this-&amp;gt;bias_term_) {
        const Dtype* bias = this-&amp;gt;blobs_[1]-&amp;gt;cpu_data();
        this-&amp;gt;forward_cpu_bias(top_data + n * this-&amp;gt;top_dim_, bias);
      }
    }
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;可以看到其实这里面他调用了forward_cpu_gemm，而这个函数内部又调用了math_function里面的caffe_cpu_gemm的通用矩阵相乘接口，GEMM的全称是General Matrix Matrix Multiply。其基本形式如下：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;C=alpha∗op(A)∗op(B)+beta∗C,&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;template &amp;lt;typename Dtype&amp;gt;
void ConvolutionLayer&amp;lt;Dtype&amp;gt;::Backward_cpu(const vector&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt;&amp;amp; top,
      const vector&amp;lt;bool&amp;gt;&amp;amp; propagate_down, const vector&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt;&amp;amp; bottom) {
   //反向传播梯度误差
  const Dtype* weight = this-&amp;gt;blobs_[0]-&amp;gt;cpu_data();
  Dtype* weight_diff = this-&amp;gt;blobs_[0]-&amp;gt;mutable_cpu_diff();
  for (int i = 0; i &amp;lt; top.size(); ++i) {
    const Dtype* top_diff = top[i]-&amp;gt;cpu_diff();
    const Dtype* bottom_data = bottom[i]-&amp;gt;cpu_data();
    Dtype* bottom_diff = bottom[i]-&amp;gt;mutable_cpu_diff();
    
    //如果有bias项，计算Bias导数
    if (this-&amp;gt;bias_term_ &amp;amp;&amp;amp; this-&amp;gt;param_propagate_down_[1]) {
      Dtype* bias_diff = this-&amp;gt;blobs_[1]-&amp;gt;mutable_cpu_diff();
      for (int n = 0; n &amp;lt; this-&amp;gt;num_; ++n) {
        this-&amp;gt;backward_cpu_bias(bias_diff, top_diff + n * this-&amp;gt;top_dim_);
      }
    }
    //计算weight
    if (this-&amp;gt;param_propagate_down_[0] || propagate_down[i]) {
      for (int n = 0; n &amp;lt; this-&amp;gt;num_; ++n) {
        // 计算weights权重的梯度
        if (this-&amp;gt;param_propagate_down_[0]) {
          this-&amp;gt;weight_cpu_gemm(bottom_data + n * this-&amp;gt;bottom_dim_,
              top_diff + n * this-&amp;gt;top_dim_, weight_diff);
        }
        //计算botttom数据的梯度，向后传递
        if (propagate_down[i]) {
          this-&amp;gt;backward_cpu_gemm(top_diff + n * this-&amp;gt;top_dim_, weight,
              bottom_diff + n * this-&amp;gt;bottom_dim_);
        }
      }
    }
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;neuron-layer&quot;&gt;Neuron Layer&lt;/h2&gt;
&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Neuron Layer&lt;/code&gt;，顾名思义这里就是神经元，激活函数的相应层。我们知道在blob进入激活函数之前和之后他的size是不会变的，而且激活值也就是输出 y 只依赖于相应的输入 x 。在Caffe里面所有的layer的实现都放在src文件夹下的layer文件夹中，基本上很多文章里应用到的layer类型它都有cpu和cuda的实现。&lt;/p&gt;

&lt;p&gt;在caffe里面NeuronLayer比较多，在此罗列了一下&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;AbsValLayer&lt;/li&gt;
  &lt;li&gt;BNLLLayer&lt;/li&gt;
  &lt;li&gt;DropoutLayer&lt;/li&gt;
  &lt;li&gt;ExpLayer&lt;/li&gt;
  &lt;li&gt;LogLayer&lt;/li&gt;
  &lt;li&gt;PowerLayer&lt;/li&gt;
  &lt;li&gt;ReLULayer&lt;/li&gt;
  &lt;li&gt;CuDNNReLULayer&lt;/li&gt;
  &lt;li&gt;SigmoidLayer&lt;/li&gt;
  &lt;li&gt;CuDNNSigmoidLayer&lt;/li&gt;
  &lt;li&gt;TanHLayer&lt;/li&gt;
  &lt;li&gt;CuDNNTanHLayer&lt;/li&gt;
  &lt;li&gt;ThresholdLayer&lt;/li&gt;
  &lt;li&gt;PReLULayer&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;relulayer&quot;&gt;ReLULayer&lt;/h3&gt;
&lt;p&gt;目前在激活层的函数中使用ReLU是非常普遍的，一般我们在看资料或者讲义中总是提到的是Sigmoid函数，它比Sigmoid有更快的收敛性，因为sigmoid在收敛的时候越靠近目标点收敛的速度会越慢，也是其函数的曲线形状决定的。而&lt;code class=&quot;highlighter-rouge&quot;&gt;ReLULayer&lt;/code&gt;则相对收敛更快，具体可以看Krizhevsky 12年的那篇ImageNet CNN文章有更详细的介绍。&lt;/p&gt;

&lt;p&gt;其在cafffe中的forward和backward函数为&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;template &amp;lt;typename Dtype&amp;gt;
void ReLULayer&amp;lt;Dtype&amp;gt;::Forward_cpu(const vector&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt;&amp;amp; bottom,
    const vector&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt;&amp;amp; top) {
  const Dtype* bottom_data = bottom[0]-&amp;gt;cpu_data();
  Dtype* top_data = top[0]-&amp;gt;mutable_cpu_data();
  const int count = bottom[0]-&amp;gt;count();
  Dtype negative_slope = this-&amp;gt;layer_param_.relu_param().negative_slope();
  for (int i = 0; i &amp;lt; count; ++i) {
    top_data[i] = std::max(bottom_data[i], Dtype(0))
        + negative_slope * std::min(bottom_data[i], Dtype(0));
  }
}

template &amp;lt;typename Dtype&amp;gt;
void ReLULayer&amp;lt;Dtype&amp;gt;::Backward_cpu(const vector&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt;&amp;amp; top,
    const vector&amp;lt;bool&amp;gt;&amp;amp; propagate_down,
    const vector&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt;&amp;amp; bottom) {
  if (propagate_down[0]) {
    const Dtype* bottom_data = bottom[0]-&amp;gt;cpu_data();
    const Dtype* top_diff = top[0]-&amp;gt;cpu_diff();
    Dtype* bottom_diff = bottom[0]-&amp;gt;mutable_cpu_diff();
    const int count = bottom[0]-&amp;gt;count();
    Dtype negative_slope = this-&amp;gt;layer_param_.relu_param().negative_slope();
    for (int i = 0; i &amp;lt; count; ++i) {
      bottom_diff[i] = top_diff[i] * ((bottom_data[i] &amp;gt; 0)
          + negative_slope * (bottom_data[i] &amp;lt;= 0));
    }
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;其余包括Sigmoid可以参考&lt;a href=&quot;http://www.cnblogs.com/louyihang-loves-baiyan/p/5200850.html&quot;&gt;Caffe源码解析6：Neuron_Layer&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;dropout-layer&quot;&gt;Dropout Layer&lt;/h3&gt;
&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Dropout Layer&lt;/code&gt;现在是非常常用的一种网络层，只用在训练阶段，一般用在网络的全连接层中，可以减少网络的过拟合问题。其思想是在训练过程中随机的将一部分输入x之置为0。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/dropout_formula.png&quot; alt=&quot;Dropout Formula&quot; /&gt;&lt;/p&gt;

&lt;p&gt;其forward_cpu和backward_cpu为:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;template &amp;lt;typename Dtype&amp;gt;
void DropoutLayer&amp;lt;Dtype&amp;gt;::Forward_cpu(const vector&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt;&amp;amp; bottom,
    const vector&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt;&amp;amp; top) {
  const Dtype* bottom_data = bottom[0]-&amp;gt;cpu_data();
  Dtype* top_data = top[0]-&amp;gt;mutable_cpu_data();
  unsigned int* mask = rand_vec_.mutable_cpu_data();
  const int count = bottom[0]-&amp;gt;count();
  if (this-&amp;gt;phase_ == TRAIN) {
    // Create random numbers构造随机数，这里是通过向量掩码来和bottom的数据相乘，scale_是控制undropped的比例
    caffe_rng_bernoulli(count, 1. - threshold_, mask);
    for (int i = 0; i &amp;lt; count; ++i) {
      top_data[i] = bottom_data[i] * mask[i] * scale_;
    }
  } else {
    caffe_copy(bottom[0]-&amp;gt;count(), bottom_data, top_data);
  }
}

template &amp;lt;typename Dtype&amp;gt;
void DropoutLayer&amp;lt;Dtype&amp;gt;::Backward_cpu(const vector&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt;&amp;amp; top,
    const vector&amp;lt;bool&amp;gt;&amp;amp; propagate_down,
    const vector&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt;&amp;amp; bottom) {
  if (propagate_down[0]) {
    const Dtype* top_diff = top[0]-&amp;gt;cpu_diff();
    Dtype* bottom_diff = bottom[0]-&amp;gt;mutable_cpu_diff();
    if (this-&amp;gt;phase_ == TRAIN) {
      const unsigned int* mask = rand_vec_.cpu_data();
      const int count = bottom[0]-&amp;gt;count();
      for (int i = 0; i &amp;lt; count; ++i) {
        bottom_diff[i] = top_diff[i] * mask[i] * scale_;
      }
    } else {
      caffe_copy(top[0]-&amp;gt;count(), top_diff, bottom_diff);
    }
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;polling-layer&quot;&gt;Polling Layer&lt;/h3&gt;
&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Pooling&lt;/code&gt;层一般在网络中是跟在Conv卷积层之后，做采样操作，其实是为了进一步缩小feature map，同时也能增大神经元的视野。在Caffe中，pooling层属于vision_layer的一部分，其相关的定义也在vision_layer.hpp的头文件中。Pooling层的相关操作比较少，在Caffe的自带模式下只有Max pooling和Average poooling两种&lt;/p&gt;

&lt;p&gt;caffe中Pooling的操作相对比较少，结构也简单，首先看它的Forward_cpu函数，在forward的时候根据相应的Pooling_method选择相应的pooling方法&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;void PoolingLayer&amp;lt;Dtype&amp;gt;::Forward_cpu(const vector&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt;&amp;amp; bottom,
      const vector&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt;&amp;amp; top) {
  const Dtype* bottom_data = bottom[0]-&amp;gt;cpu_data();
  Dtype* top_data = top[0]-&amp;gt;mutable_cpu_data();
  const int top_count = top[0]-&amp;gt;count();
  //将mask信息输出到top[1],如果top大于1
  const bool use_top_mask = top.size() &amp;gt; 1;
  int* mask = NULL;  // suppress warnings about uninitalized variables
  Dtype* top_mask = NULL;
  switch (this-&amp;gt;layer_param_.pooling_param().pool()) {
  case PoolingParameter_PoolMethod_MAX://这里的case主要是实现max pooling的方法
    // Initialize
    if (use_top_mask) {
      top_mask = top[1]-&amp;gt;mutable_cpu_data();
      caffe_set(top_count, Dtype(-1), top_mask);
    } else {
      mask = max_idx_.mutable_cpu_data();
      caffe_set(top_count, -1, mask);
    }
    caffe_set(top_count, Dtype(-FLT_MAX), top_data);
    // The main loop
    for (int n = 0; n &amp;lt; bottom[0]-&amp;gt;num(); ++n) {
      for (int c = 0; c &amp;lt; channels_; ++c) {
        for (int ph = 0; ph &amp;lt; pooled_height_; ++ph) {
          for (int pw = 0; pw &amp;lt; pooled_width_; ++pw) {
            int hstart = ph * stride_h_ - pad_h_;//这里的hstart，wstart,hend,wend指的是pooling窗口在特征图中的坐标，对应左上右下即x1 y1 x2 y2
            int wstart = pw * stride_w_ - pad_w_;
            int hend = min(hstart + kernel_h_, height_);
            int wend = min(wstart + kernel_w_, width_);
            hstart = max(hstart, 0);
            wstart = max(wstart, 0);
            const int pool_index = ph * pooled_width_ + pw;
            for (int h = hstart; h &amp;lt; hend; ++h) {
              for (int w = wstart; w &amp;lt; wend; ++w) {
                const int index = h * width_ + w;//记录index偏差
                if (bottom_data[index] &amp;gt; top_data[pool_index]) {//不停迭代
                  top_data[pool_index] = bottom_data[index];
                  if (use_top_mask) {
                    top_mask[pool_index] = static_cast&amp;lt;Dtype&amp;gt;(index);//记录当前最大值的的坐标索引
                  } else {
                    mask[pool_index] = index;
                  }
                }
              }
            }
          }
        }
        // 计算偏移量，进入下一张图的index起始地址
        bottom_data += bottom[0]-&amp;gt;offset(0, 1);
        top_data += top[0]-&amp;gt;offset(0, 1);
        if (use_top_mask) {
          top_mask += top[0]-&amp;gt;offset(0, 1);
        } else {
          mask += top[0]-&amp;gt;offset(0, 1);
        }
      }
    }
    break;
 case PoolingParameter_PoolMethod_AVE://average_pooling
    for (int i = 0; i &amp;lt; top_count; ++i) {
      top_data[i] = 0;
    }
    // The main loop
    for (int n = 0; n &amp;lt; bottom[0]-&amp;gt;num(); ++n) {//同样是主循环
      for (int c = 0; c &amp;lt; channels_; ++c) {
        for (int ph = 0; ph &amp;lt; pooled_height_; ++ph) {
          for (int pw = 0; pw &amp;lt; pooled_width_; ++pw) {
            int hstart = ph * stride_h_ - pad_h_;
            int wstart = pw * stride_w_ - pad_w_;
            int hend = min(hstart + kernel_h_, height_ + pad_h_);
            int wend = min(wstart + kernel_w_, width_ + pad_w_);
            int pool_size = (hend - hstart) * (wend - wstart);
            hstart = max(hstart, 0);
            wstart = max(wstart, 0);
            hend = min(hend, height_);
            wend = min(wend, width_);
            for (int h = hstart; h &amp;lt; hend; ++h) {
              for (int w = wstart; w &amp;lt; wend; ++w) {
                top_data[ph * pooled_width_ + pw] +=
                    bottom_data[h * width_ + w];
              }
            }
            top_data[ph * pooled_width_ + pw] /= pool_size;//获得相应的平均值
          }
        }
        // compute offset同理计算下一个图的起始地址
        bottom_data += bottom[0]-&amp;gt;offset(0, 1);
        top_data += top[0]-&amp;gt;offset(0, 1);
      }
    }
    break;
  case PoolingParameter_PoolMethod_STOCHASTIC:
    NOT_IMPLEMENTED;
    break;
  default:
    LOG(FATAL) &amp;lt;&amp;lt; &quot;Unknown pooling method.&quot;;
  }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;对于误差的反向传导
对于pooling层的误差传到，根据下式&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/pooling_formula.png&quot; alt=&quot;Pooling Formula&quot; /&gt;&lt;/p&gt;

&lt;p&gt;这里的Upsample具体可以根据相应的pooling方法来进行上采样，upsample的基本思想也是将误差进行的平摊到各个采样的对应点上。在这里pooling因为是线性的所以h这一项其实是可以省略的。&lt;/p&gt;

&lt;p&gt;具体的计算推导过程请结合&lt;a href=&quot;http://www.cnblogs.com/tornadomeet/p/3468450.html&quot;&gt;http://www.cnblogs.com/tornadomeet/p/3468450.html&lt;/a&gt;有详细的推导过程，结合代码中主循环中的最里项会更清晰的明白&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;template &amp;lt;typename Dtype&amp;gt;
void PoolingLayer&amp;lt;Dtype&amp;gt;::Backward_cpu(const vector&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt;&amp;amp; top,
      const vector&amp;lt;bool&amp;gt;&amp;amp; propagate_down, const vector&amp;lt;Blob&amp;lt;Dtype&amp;gt;*&amp;gt;&amp;amp; bottom) {
  if (!propagate_down[0]) {
    return;
  }
  const Dtype* top_diff = top[0]-&amp;gt;cpu_diff();//首先获得上层top_blob的diff
  Dtype* bottom_diff = bottom[0]-&amp;gt;mutable_cpu_diff();
  caffe_set(bottom[0]-&amp;gt;count(), Dtype(0), bottom_diff);
  // We'll output the mask to top[1] if it's of size &amp;gt;1.
  const bool use_top_mask = top.size() &amp;gt; 1;
  const int* mask = NULL;  // suppress warnings about uninitialized variables
  const Dtype* top_mask = NULL;
  switch (this-&amp;gt;layer_param_.pooling_param().pool()) {
  case PoolingParameter_PoolMethod_MAX:
    // The main loop
    if (use_top_mask) {
      top_mask = top[1]-&amp;gt;cpu_data();
    } else {
      mask = max_idx_.cpu_data();
    }
    for (int n = 0; n &amp;lt; top[0]-&amp;gt;num(); ++n) {
      for (int c = 0; c &amp;lt; channels_; ++c) {
        for (int ph = 0; ph &amp;lt; pooled_height_; ++ph) {
          for (int pw = 0; pw &amp;lt; pooled_width_; ++pw) {
            const int index = ph * pooled_width_ + pw;
            const int bottom_index =
                use_top_mask ? top_mask[index] : mask[index];//根据max pooling记录的mask位置，进行误差反转
            bottom_diff[bottom_index] += top_diff[index];
          }
        }
        bottom_diff += bottom[0]-&amp;gt;offset(0, 1);
        top_diff += top[0]-&amp;gt;offset(0, 1);
        if (use_top_mask) {
          top_mask += top[0]-&amp;gt;offset(0, 1);
        } else {
          mask += top[0]-&amp;gt;offset(0, 1);
        }
      }
    }
    break;
  case PoolingParameter_PoolMethod_AVE:
    // The main loop
    for (int n = 0; n &amp;lt; top[0]-&amp;gt;num(); ++n) {
      for (int c = 0; c &amp;lt; channels_; ++c) {
        for (int ph = 0; ph &amp;lt; pooled_height_; ++ph) {
          for (int pw = 0; pw &amp;lt; pooled_width_; ++pw) {
            int hstart = ph * stride_h_ - pad_h_;
            int wstart = pw * stride_w_ - pad_w_;
            int hend = min(hstart + kernel_h_, height_ + pad_h_);
            int wend = min(wstart + kernel_w_, width_ + pad_w_);
            int pool_size = (hend - hstart) * (wend - wstart);
            hstart = max(hstart, 0);
            wstart = max(wstart, 0);
            hend = min(hend, height_);
            wend = min(wend, width_);
            for (int h = hstart; h &amp;lt; hend; ++h) {
              for (int w = wstart; w &amp;lt; wend; ++w) {
                bottom_diff[h * width_ + w] +=
                  top_diff[ph * pooled_width_ + pw] / pool_size;//mean_pooling中，bottom的误差值按pooling窗口中的大小计算，从上一层进行填充后，再除窗口大小
              }
            }
          }
        }
        // offset
        bottom_diff += bottom[0]-&amp;gt;offset(0, 1);
        top_diff += top[0]-&amp;gt;offset(0, 1);
      }
    }
    break;
  case PoolingParameter_PoolMethod_STOCHASTIC:
    NOT_IMPLEMENTED;
    break;
  default:
    LOG(FATAL) &amp;lt;&amp;lt; &quot;Unknown pooling method.&quot;;
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>Jim Chen</name></author><summary type="html">模块学习——Layer</summary></entry><entry><title type="html">Caffe Learning Notes(3)</title><link href="http://localhost:4000/posts/2017-07-04-Caffe-Learning-Notes(3)" rel="alternate" type="text/html" title="Caffe Learning Notes(3)" /><published>2017-07-04T17:00:00+08:00</published><updated>2017-07-04T17:00:00+08:00</updated><id>http://localhost:4000/posts/Caffe-Learning-Notes(3)</id><content type="html" xml:base="http://localhost:4000/posts/2017-07-04-Caffe-Learning-Notes(3)">&lt;h1 id=&quot;模块学习blob类&quot;&gt;模块学习——Blob类&lt;/h1&gt;

&lt;p&gt;主要参考自 &lt;a href=&quot;http://www.cnblogs.com/louyihang-loves-baiyan/&quot;&gt;楼燚(yì)航的blog——Caffe源码解析1：Blob&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;下面会对具体的代码进行分析，但是如果对整体代码有想参考的部分，可以参考另外一篇博客：&lt;a href=&quot;http://blog.csdn.net/xizero00/article/details/50886829&quot;&gt;caffe 中 BLOB的实现
&lt;/a&gt;，其中对整个代码部分进行了注释，可以参考学习。&lt;/p&gt;

&lt;p&gt;首先提一下&lt;code class=&quot;highlighter-rouge&quot;&gt;explicit&lt;/code&gt;关键字的作用是禁止单参数构造函数的隐式转换。&lt;code class=&quot;highlighter-rouge&quot;&gt;iniline&lt;/code&gt;主要是将代码进行复制，扩充，会使代码总量上升，好处就是可以节省调用的开销，能提高执行效率。&lt;/p&gt;

&lt;p&gt;实际上&lt;code class=&quot;highlighter-rouge&quot;&gt;Blob&lt;/code&gt;包含了三类数据&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;data，前向传播所用到的数据&lt;/li&gt;
  &lt;li&gt;diff，反向传播所用到的数据&lt;/li&gt;
  &lt;li&gt;shape，解释data和diff的shape数据&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;主要变量&quot;&gt;主要变量&lt;/h2&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;shared_ptr&amp;lt;SyncedMemory&amp;gt; data_;
shared_ptr&amp;lt;SyncedMemory&amp;gt; diff_;
shared_ptr&amp;lt;SyncedMemory&amp;gt; shape_data_;
vector&amp;lt;int&amp;gt; shape_;
int count_;
int capacity_;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Blob只是一个基本的数据结构，因此内部的变量相对较少，首先是 &lt;code class=&quot;highlighter-rouge&quot;&gt;data_&lt;/code&gt; 指针，指针类型是 &lt;code class=&quot;highlighter-rouge&quot;&gt;shared_ptr&lt;/code&gt; ，属于 &lt;code class=&quot;highlighter-rouge&quot;&gt;boost&lt;/code&gt; 库的一个智能指针，这一部分主要用来申请内存存储 &lt;code class=&quot;highlighter-rouge&quot;&gt;data&lt;/code&gt; ， &lt;code class=&quot;highlighter-rouge&quot;&gt;data&lt;/code&gt; 主要是正向传播的时候用的。同理， &lt;code class=&quot;highlighter-rouge&quot;&gt;diff_&lt;/code&gt; 主要用来存储偏差，update data ， &lt;code class=&quot;highlighter-rouge&quot;&gt;shape_data&lt;/code&gt; 和 &lt;code class=&quot;highlighter-rouge&quot;&gt;shape_&lt;/code&gt; 都是存储Blob的形状，一个是老版本一个是新版本。 &lt;code class=&quot;highlighter-rouge&quot;&gt;count&lt;/code&gt; 表示Blob中的元素个数，也就是 &lt;code class=&quot;highlighter-rouge&quot;&gt;个数*通道数*高度*宽度&lt;/code&gt; , &lt;code class=&quot;highlighter-rouge&quot;&gt;capacity&lt;/code&gt; 表示当前的元素个数，因为&lt;code class=&quot;highlighter-rouge&quot;&gt;Blob&lt;/code&gt;可能会reshape。&lt;/p&gt;

&lt;h2 id=&quot;主要函数&quot;&gt;主要函数&lt;/h2&gt;
&lt;h3 id=&quot;reshape&quot;&gt;Reshape()&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  void Reshape(const int num, const int channels, const int height, const int width);
  void Reshape(const vector&amp;lt;int&amp;gt;&amp;amp; shape);
  void Reshape(const BlobShape&amp;amp; shape);
  void ReshapeLike(const Blob&amp;amp; other);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Blob中除了基础的构造函数，还有大量的 &lt;code class=&quot;highlighter-rouge&quot;&gt;Reshape&lt;/code&gt; 函数， &lt;code class=&quot;highlighter-rouge&quot;&gt;Reshape&lt;/code&gt; 函数在Layer中的 &lt;code class=&quot;highlighter-rouge&quot;&gt;reshape&lt;/code&gt; 或者 &lt;code class=&quot;highlighter-rouge&quot;&gt;forward&lt;/code&gt; 操作中来adjust dimension。同时在改变&lt;code class=&quot;highlighter-rouge&quot;&gt;Blob&lt;/code&gt;大小时，内存将会被重新分配，如果内存大小不够了，并且额外的内存将不会被释放。对input的blob进行reshape,如果立马调用 &lt;code class=&quot;highlighter-rouge&quot;&gt;Net::Backward&lt;/code&gt; 是会出错的，因为reshape之后，要么 &lt;code class=&quot;highlighter-rouge&quot;&gt;Net::forward&lt;/code&gt; 或者 &lt;code class=&quot;highlighter-rouge&quot;&gt;Net::Reshape&lt;/code&gt; 就会被调用来将新的input shape传播到高层。&lt;/p&gt;

&lt;h3 id=&quot;count&quot;&gt;count()&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;inline int shape(int index) const {
    return shape_[CanonicalAxisIndex(index)];
  }
  inline int num_axes() const { return shape_.size(); }
  inline int count() const { return count_; }
  inline int count(int start_axis, int end_axis) const {
    ...
  }
  inline int count(int start_axis) const {
    return count(start_axis, num_axes());
  }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Blob类里面有重载很多个count()函数，主要还是为了统计Blob的容量（volume），或者是某一片（slice），从某个axis到具体某个axis的shape乘积。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;并且Blob的Index是可以从负坐标开始读的，这一点跟Python好像&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;对于Blob中的4个基本变量 &lt;code class=&quot;highlighter-rouge&quot;&gt;num, channel, height, width&lt;/code&gt; 可以直接通过shape(0),shape(1),shape(2),shape(3)来访问。&lt;/p&gt;

&lt;h3 id=&quot;data--diff&quot;&gt;Data &amp;amp; Diff&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;inline Dtype data_at(const int n, const int c, const int h, const int w)
inline Dtype diff_at(const int n, const int c, const int h, const int w)
inline Dtype data_at(const vector&amp;lt;int&amp;gt;&amp;amp; index)
inline Dtype diff_at(const vector&amp;lt;int&amp;gt;&amp;amp; index)
inline const shared_ptr&amp;lt;SyncedMemory&amp;gt;&amp;amp; data()
inline const shared_ptr&amp;lt;SyncedMemory&amp;gt;&amp;amp; diff()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;这一部分函数主要通过给定的位置访问数据，根据位置计算与数据起始的偏差offset，在通过cpu_data*指针获得地址。下面几个函数都是获得&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;const Dtype* cpu_data() const;
void set_cpu_data(Dtype* data);
const int* gpu_shape() const;
const Dtype* gpu_data() const;
const Dtype* cpu_diff() const;
const Dtype* gpu_diff() const;
Dtype* mutable_cpu_data();
Dtype* mutable_gpu_data();
Dtype* mutable_cpu_diff();
Dtype* mutable_gpu_diff();
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;可以看到这里有data和diff两类数据，&lt;strong&gt;而这个diff就是我们所熟知的偏差，前者主要存储前向传递的数据，而后者存储的是反向传播中的梯度&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;update&quot;&gt;Update()&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;void Blob&amp;lt;Dtype&amp;gt;::Update() {
  // We will perform update based on where the data is located.
  switch (data_-&amp;gt;head()) {
  case SyncedMemory::HEAD_AT_CPU:
    // perform computation on CPU
    caffe_axpy&amp;lt;Dtype&amp;gt;(count_, Dtype(-1),
        static_cast&amp;lt;const Dtype*&amp;gt;(diff_-&amp;gt;cpu_data()),
        static_cast&amp;lt;Dtype*&amp;gt;(data_-&amp;gt;mutable_cpu_data()));
    break;
  case SyncedMemory::HEAD_AT_GPU:
  case SyncedMemory::SYNCED:
#ifndef CPU_ONLY
    // perform computation on GPU
    caffe_gpu_axpy&amp;lt;Dtype&amp;gt;(count_, Dtype(-1),
        static_cast&amp;lt;const Dtype*&amp;gt;(diff_-&amp;gt;gpu_data()),
        static_cast&amp;lt;Dtype*&amp;gt;(data_-&amp;gt;mutable_gpu_data()));
#else
    NO_GPU;
#endif
    break;
  default:
    LOG(FATAL) &amp;lt;&amp;lt; &quot;Syncedmem not initialized.&quot;;
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;这个里面核心的一句话就是&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;caffe_axpy&amp;lt;Dtype&amp;gt;(count_, Dtype(-1),
        static_cast&amp;lt;const Dtype*&amp;gt;(diff_-&amp;gt;cpu_data()),
        static_cast&amp;lt;Dtype*&amp;gt;(data_-&amp;gt;mutable_cpu_data()));  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;这句话是在 &lt;code class=&quot;highlighter-rouge&quot;&gt;include/caffe/util/math_function.cpp&lt;/code&gt; 文件中&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;void caffe_axpy&amp;lt;float&amp;gt;(const int N, const float alpha, const float* X,
    float* Y) { cblas_saxpy(N, alpha, X, 1, Y, 1); }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;可以看到这一段调用了cblas库的方法&lt;/strong&gt;。实现的功能是&lt;/p&gt;

&lt;center&gt;
Y=alpha∗X+beta∗Y
&lt;/center&gt;

&lt;p&gt;也就是blob里面的data部分减去diff部分&lt;/p&gt;

&lt;h3 id=&quot;norm&quot;&gt;norm&lt;/h3&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Dtype asum_data() const;//计算data的L1范数
Dtype asum_diff() const;//计算diff的L1范数
Dtype sumsq_data() const;//计算data的L2范数
Dtype sumsq_diff() const;//计算diff的L2范数
void scale_data(Dtype scale_factor);//将data部分乘以一个因子
void scale_diff(Dtype scale_factor);//将diff部分乘一个因子
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;模块学习syncedmemory&quot;&gt;模块学习——SyncedMemory&lt;/h1&gt;
&lt;p&gt;看到SyncedMem就知道，这是在做内存同步的操作。这类个类的代码比较少，但是作用是非常明显的。文件对应着syncedmem.hpp,着syncedmem.cpp&lt;/p&gt;

&lt;p&gt;首先是两个全局的内联函数。如果机器是支持GPU的并且安装了cuda，通过cudaMallocHost分配的host memory将会被pinned，&lt;strong&gt;这里我谷歌了一下，pinned的意思就是内存不会被paged out，我们知道内存里面是由页作为基本的管理单元。分配的内存可以常驻在内存空间中对效率是有帮助的，空间不会被别的进程所抢占。&lt;/strong&gt;同样如果内存越大，能被分配的Pinned内存自然也越大。&lt;strong&gt;还有一点是，对于单一的GPU而言提升并不会太显著，但是对于多个GPU的并行而言可以显著提高稳定性。&lt;/strong&gt; [1]&lt;/p&gt;

&lt;h2 id=&quot;inference&quot;&gt;Inference&lt;/h2&gt;
&lt;p&gt;[1] http://www.cnblogs.com/louyihang-loves-baiyan/p/5150554.html&lt;/p&gt;</content><author><name>Jim Chen</name></author><summary type="html">模块学习——Blob类</summary></entry><entry><title type="html">Caffe Learning Notes(2)</title><link href="http://localhost:4000/posts/2017-07-04-Caffe-Learning-Notes(2)" rel="alternate" type="text/html" title="Caffe Learning Notes(2)" /><published>2017-07-04T12:00:00+08:00</published><updated>2017-07-04T12:00:00+08:00</updated><id>http://localhost:4000/posts/Caffe-Learning-Notes(2)</id><content type="html" xml:base="http://localhost:4000/posts/2017-07-04-Caffe-Learning-Notes(2)">&lt;h1 id=&quot;模块学习protocol-buffer&quot;&gt;模块学习——Protocol Buffer&lt;/h1&gt;
&lt;p&gt;除了清晰的代码结构，让Caffe变得易用更应该归功于Google Protocol Buffer的使用。Google Protocol Buffer是Google开发的一个用于serializing结构化数据的开源工具:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Protocol buffers are a language-neutral, platform-neutral extensible mechanism for serializing structured data.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Caffe使用这个工具来定义&lt;code class=&quot;highlighter-rouge&quot;&gt;Solver&lt;/code&gt;和&lt;code class=&quot;highlighter-rouge&quot;&gt;Net&lt;/code&gt;，以及&lt;code class=&quot;highlighter-rouge&quot;&gt;Net&lt;/code&gt;中每一个&lt;code class=&quot;highlighter-rouge&quot;&gt;layer&lt;/code&gt;的参数。这使得只是想使用Caffe目前支持的&lt;code class=&quot;highlighter-rouge&quot;&gt;Layer&lt;/code&gt;(已经非常丰富了)来做一些实验或者demo的用户可以不去和代码打交道，只需要在&lt;code class=&quot;highlighter-rouge&quot;&gt;*.prototxt&lt;/code&gt;文件中描述自己的&lt;code class=&quot;highlighter-rouge&quot;&gt;Solver&lt;/code&gt;和&lt;code class=&quot;highlighter-rouge&quot;&gt;Net&lt;/code&gt;即可，再通过Caffe提供的command line interfaces就可以完成模型的train, finetune, test等功能。[1]&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Protocol Buffer的学习&lt;/strong&gt; &lt;a href=&quot;https://developers.google.com/protocol-buffers/docs/cpptutorial#why-use-protocol-buffers&quot;&gt;Protocol Buffer Basics: C++&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;caffe中的应用&quot;&gt;Caffe中的应用&lt;/h3&gt;
&lt;p&gt;caffe当中的使用可以见 &lt;code class=&quot;highlighter-rouge&quot;&gt;caffe/src/caffe/proto/caffe.proto&lt;/code&gt;&lt;br /&gt;
Reference: &lt;a href=&quot;http://alanse7en.github.io/caffedai-ma-jie-xi-2/&quot;&gt;Protocol Buffer in Caffe&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;prototxt的编写可以直接参考官方文档，可以参考的地方也很多，编写时也有一个可视化的工具来帮梦，地址入下：
&lt;a href=&quot;http://ethereon.github.io/netscope/quickstart.html&quot;&gt;Quick Start — Netscope&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;
&lt;p&gt;[1] http://alanse7en.github.io/caffedai-ma-jie-xi-1/&lt;/p&gt;

&lt;h1 id=&quot;模块学习command-line-interfaces&quot;&gt;模块学习——Command Line Interfaces&lt;/h1&gt;
&lt;h2 id=&quot;google-flags&quot;&gt;Google Flags&lt;/h2&gt;
&lt;p&gt;Caffe的Command Line Interfaces一共提供了四个功能：&lt;code class=&quot;highlighter-rouge&quot;&gt;train&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;test&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;time&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;device_query&lt;/code&gt;，而Interfaces的输入除了这四种功能还可以输入诸如&lt;code class=&quot;highlighter-rouge&quot;&gt;-solver&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;-weights&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;-snapshot&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;-gpu&lt;/code&gt;等参数。 &lt;em&gt;这些参数的解析是通过Google Flags这个工具来完成的&lt;/em&gt; 。[1]&lt;/p&gt;

&lt;p&gt;解析这些标志的代码在caffe.cpp中的main()中调用了/CAFFE_ROOT/src/common.cpp中的GlobalInit(&amp;amp;argc, &amp;amp;argv)函数：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;void GlobalInit(int* pargc, char*** pargv) {
  // Google flags.
  ::gflags::ParseCommandLineFlags(pargc, pargv, true);
  // Google logging.
  ::google::InitGoogleLogging(*(pargv)[0]);
  // Provide a backtrace on segfault.
  ::google::InstallFailureSignalHandler();
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;register-brew-function&quot;&gt;Register Brew Function&lt;/h2&gt;
&lt;p&gt;Caffe在Command Line Interfaces中一共提供了4种功能:&lt;code class=&quot;highlighter-rouge&quot;&gt;train&lt;/code&gt;/&lt;code class=&quot;highlighter-rouge&quot;&gt;test&lt;/code&gt;/&lt;code class=&quot;highlighter-rouge&quot;&gt;time&lt;/code&gt;/&lt;code class=&quot;highlighter-rouge&quot;&gt;device_query&lt;/code&gt;，分别对应着四个函数，这四个函数的调用是通过一个叫做&lt;code class=&quot;highlighter-rouge&quot;&gt;g_brew_map&lt;/code&gt;的全局变量来完成的：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;// A simple registry for caffe commands.
typedef int (*BrewFunction)();
typedef std::map&amp;lt;caffe::string, BrewFunction&amp;gt; BrewMap;
BrewMap g_brew_map;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;g_brew_map&lt;/code&gt;是一个key为&lt;code class=&quot;highlighter-rouge&quot;&gt;string&lt;/code&gt;类型，value为&lt;code class=&quot;highlighter-rouge&quot;&gt;BrewFunction&lt;/code&gt;类型的一个map类型的全局变量，&lt;code class=&quot;highlighter-rouge&quot;&gt;BrewFunction&lt;/code&gt;是一个函数指针类型，指向的是参数为空，返回值为int的函数，也就是&lt;code class=&quot;highlighter-rouge&quot;&gt;train&lt;/code&gt;/&lt;code class=&quot;highlighter-rouge&quot;&gt;test&lt;/code&gt;/&lt;code class=&quot;highlighter-rouge&quot;&gt;time&lt;/code&gt;/&lt;code class=&quot;highlighter-rouge&quot;&gt;device_query&lt;/code&gt;这四个函数的类型。在&lt;code class=&quot;highlighter-rouge&quot;&gt;train&lt;/code&gt;等四个函数实现的后面都紧跟着这样一句宏的调用：RegisterBrewFunction(train);&lt;/p&gt;

&lt;p&gt;总结一下：RegisterBrewFunction这个宏在每一个实现主要功能的函数之后将这个函数的名字和其对应的函数指针添加到了&lt;code class=&quot;highlighter-rouge&quot;&gt;g_brew_map&lt;/code&gt;中，然后在main函数中，通过&lt;code class=&quot;highlighter-rouge&quot;&gt;GetBrewFunction&lt;/code&gt;得到了我们需要调用的那个函数的函数指针，并完成了调用。&lt;/p&gt;

&lt;p&gt;具体可以参考 reference [1] &lt;a href=&quot;http://alanse7en.github.io/caffedai-ma-3/&quot;&gt;Caffe代码解析(3) – Xuesong’s Blog&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;train-函数&quot;&gt;train() 函数&lt;/h2&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; CHECK_GT(FLAGS_solver.size(), 0) &amp;lt;&amp;lt; &quot;Need a solver definition to train.&quot;;
CHECK(!FLAGS_snapshot.size() || !FLAGS_weights.size())
    &amp;lt;&amp;lt; &quot;Give a snapshot to resume training or weights to finetune &quot;
    &quot;but not both.&quot;;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;这段代码的第一行使用了&lt;code class=&quot;highlighter-rouge&quot;&gt;glog&lt;/code&gt;的&lt;code class=&quot;highlighter-rouge&quot;&gt;CHECK_GT&lt;/code&gt;宏（含义为check greater than），检查&lt;code class=&quot;highlighter-rouge&quot;&gt;FLAGS_solver&lt;/code&gt;的size是否大于0，如果小于或等于0则输出提示：”Need a solver definition to train”。&lt;code class=&quot;highlighter-rouge&quot;&gt;FLAGS_solver&lt;/code&gt;是最开始通过&lt;code class=&quot;highlighter-rouge&quot;&gt;DEFINE_string&lt;/code&gt;定义的标志，如果我们希望训练一个模型，那么自然应该应该提供对应的solver定义文件的路径，这一句话正是在确保我们提供了这样的路径。这样的检查语句在后续的代码中会经常出现，将不再一一详细解释，如果有不清楚含义的&lt;code class=&quot;highlighter-rouge&quot;&gt;glog&lt;/code&gt;宏可以去看看文档。 与第一行代码类似，第二行代码是确保用户没有同时提供 snapshot 和 weights 参数，这两个参数都是继续之前的训练或者进行fine-tuning的，如果同时指明了这两个标志，则不知道到底应该从哪个路径的文件去读入模型的相关参数更为合适。&lt;/p&gt;

&lt;p&gt;然后出现了 &lt;code class=&quot;highlighter-rouge&quot;&gt;SolverParameter solver_param&lt;/code&gt; 的声明和解析的代码：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;caffe::SolverParameter solver_param;
caffe::ReadSolverParamsFromTextFileOrDie(FLAGS_solver, &amp;amp;solver_param);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;SolverParameter&lt;/code&gt;是通过Google Protocol Buffer自动生成的一个类，而具体的解析函数将在下一部分具体解释。&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;接下来这一部分的代码是根据用户的设置来选择Caffe工作的模式（GPU或CPU）以及使用哪些GPU(caffe已经支持了多GPU同时工作！具体参考： [Caffe&lt;/td&gt;
      &lt;td&gt;Interfaces](http://caffe.berkeleyvision.org/tutorial/interfaces.html)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  // If the gpus flag is not provided, allow the mode and device to be set
  // in the solver prototxt.
  if (FLAGS_gpu.size() == 0
      &amp;amp;&amp;amp; solver_param.solver_mode() == caffe::SolverParameter_SolverMode_GPU) {
      if (solver_param.has_device_id()) {
          FLAGS_gpu = &quot;&quot; +
              boost::lexical_cast&amp;lt;string&amp;gt;(solver_param.device_id());
      } else {  // Set default GPU if unspecified
          FLAGS_gpu = &quot;&quot; + boost::lexical_cast&amp;lt;string&amp;gt;(0);
      }
  }

  vector&amp;lt;int&amp;gt; gpus;
  get_gpus(&amp;amp;gpus);
  if (gpus.size() == 0) {
    LOG(INFO) &amp;lt;&amp;lt; &quot;Use CPU.&quot;;
    Caffe::set_mode(Caffe::CPU);
  } else {
    ostringstream s;
    for (int i = 0; i &amp;lt; gpus.size(); ++i) {
      s &amp;lt;&amp;lt; (i ? &quot;, &quot; : &quot;&quot;) &amp;lt;&amp;lt; gpus[i];
    }
    LOG(INFO) &amp;lt;&amp;lt; &quot;Using GPUs &quot; &amp;lt;&amp;lt; s.str();
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;首先是判断用户在Command Line中是否输入了gpu相关的参数，如果没有(FLAGS_gpu.size()==0)但是用户在solver的prototxt定义中提供了相关的参数，那就把相关的参数放到FLAGS_gpu中，如果用户仅仅是选择了在solver的prototxt定义中选择了GPU模式，但是没有指明具体的gpu_id，那么就默认设置为0。&lt;/p&gt;

&lt;p&gt;接下来的代码则通过一个get_gpus的函数，将存放在FLAGS_gpu中的string转成了一个vector，并完成了具体的设置。[1]&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;p&gt;[1] http://alanse7en.github.io/caffedai-ma-3/&lt;/p&gt;</content><author><name>Jim Chen</name></author><summary type="html">模块学习——Protocol Buffer 除了清晰的代码结构，让Caffe变得易用更应该归功于Google Protocol Buffer的使用。Google Protocol Buffer是Google开发的一个用于serializing结构化数据的开源工具:</summary></entry><entry><title type="html">Caffe Learning Notes(1)</title><link href="http://localhost:4000/posts/2017-07-03-Caffe-Learning-Notes(1)" rel="alternate" type="text/html" title="Caffe Learning Notes(1)" /><published>2017-07-03T20:15:00+08:00</published><updated>2017-07-03T20:15:00+08:00</updated><id>http://localhost:4000/posts/Caffe-Learning-Notes(1)</id><content type="html" xml:base="http://localhost:4000/posts/2017-07-03-Caffe-Learning-Notes(1)">&lt;p&gt;之前一直在学习&lt;em&gt;Transfer Learning&lt;/em&gt;，看到&lt;em&gt;DAN&lt;/em&gt;和&lt;em&gt;RTN&lt;/em&gt;的源代码都是用Caffe写的，所以趁此机会来学习一下Caffe的源代码，为了之后能自己写出来自己的类。&lt;/p&gt;

&lt;p&gt;本文在全局上主要参考&lt;a href=&quot;https://buptldy.github.io/2016/10/09/2016-10-09-Caffe_Code/&quot;&gt;Caffe Source Code Analysis
&lt;/a&gt;这篇博客。这个里面对整个初始化和训练做了一个比较高层次的简单的介绍，可以通读一下来学习基本过程。&lt;/p&gt;

&lt;p&gt;接下来还主要参考了不同的学习笔记如下：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://blog.csdn.net/xizero00/article/details/50886829&quot;&gt;http://blog.csdn.net/xizero00/article/details/50886829&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://blog.csdn.net/u011104550/article/details/51232667&quot;&gt;http://blog.csdn.net/u011104550/article/details/51232667&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://www.cnblogs.com/louyihang-loves-baiyan/p/5149628.html&quot;&gt;http://www.cnblogs.com/louyihang-loves-baiyan/p/5149628.html&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://alanse7en.github.io/caffedai-ma-jie-xi-1/&quot;&gt;http://alanse7en.github.io/caffedai-ma-jie-xi-1/&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;后文对这些内容进行了总结和归纳。&lt;/p&gt;

&lt;h1 id=&quot;caffe-简介&quot;&gt;Caffe 简介&lt;/h1&gt;

&lt;p&gt;一般在介绍Caffe代码结构的时候，大家都会说Caffe主要由&lt;code class=&quot;highlighter-rouge&quot;&gt;Blob&lt;/code&gt;,&lt;code class=&quot;highlighter-rouge&quot;&gt;Layer&lt;/code&gt;,&lt;code class=&quot;highlighter-rouge&quot;&gt;Net&lt;/code&gt;和&lt;code class=&quot;highlighter-rouge&quot;&gt;Solver&lt;/code&gt;这几个部分组成。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Blob&lt;/code&gt; 主要用来表示网络中的数据，包括训练数据，网络各层自身的参数(包括权值、偏置以及它们的梯度)，网络之间传递的数据都是通过 &lt;code class=&quot;highlighter-rouge&quot;&gt;Blob&lt;/code&gt; 来实现的，同时 &lt;code class=&quot;highlighter-rouge&quot;&gt;Blob&lt;/code&gt; 数据也支持在 CPU 与 GPU 上存储，能够在两者之间做同步。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Layer&lt;/code&gt; 是对神经网络中各种层的一个抽象，包括我们熟知的卷积层和下采样层，还有全连接层和各种激活函数层等等。同时每种 &lt;code class=&quot;highlighter-rouge&quot;&gt;Layer&lt;/code&gt; 都实现了前向传播和反向传播，并通过 &lt;code class=&quot;highlighter-rouge&quot;&gt;Blob&lt;/code&gt; 来传递数据。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Net&lt;/code&gt; 是对整个网络的表示，由各种 &lt;code class=&quot;highlighter-rouge&quot;&gt;Layer&lt;/code&gt; 前后连接组合而成，也是我们所构建的网络模型。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Solver&lt;/code&gt; 定义了针对 Net 网络模型的求解方法，记录网络的训练过程，保存网络模型参数，中断并恢复网络的训练过程。自定义 &lt;code class=&quot;highlighter-rouge&quot;&gt;Solver&lt;/code&gt; 能够实现不同的网络求解方式。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/caffe_structure.png&quot; alt=&quot;Caffe structures&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;总体学习通过caffe训练lenet来看看网络初始化和训练过程&quot;&gt;总体学习——通过Caffe训练LeNet来看看网络初始化和训练过程&lt;/h2&gt;

&lt;p&gt;在Caffe提供的例子里，训练LeNet网络的命令为：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cd $CAFFE_ROOT
./build/tools/caffe train --solver=examples/mnist/lenet_solver.prototxt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;其中第一个参数&lt;code class=&quot;highlighter-rouge&quot;&gt;build/tools/caffe&lt;/code&gt;是Caffe框架的主要框架，由&lt;code class=&quot;highlighter-rouge&quot;&gt;tools/caffe.cpp&lt;/code&gt;文件编译而来，第二个参数&lt;code class=&quot;highlighter-rouge&quot;&gt;train&lt;/code&gt;表示是要训练网络，第三个参数是 &lt;code class=&quot;highlighter-rouge&quot;&gt;solver&lt;/code&gt; 的 &lt;code class=&quot;highlighter-rouge&quot;&gt;protobuf&lt;/code&gt; 描述文件。在Caffe中，网络模型的描述及其求解都是通过 &lt;code class=&quot;highlighter-rouge&quot;&gt;protobuf&lt;/code&gt; 定义的，并不需要通过敲代码来实现。同时，模型的参数也是通过 &lt;code class=&quot;highlighter-rouge&quot;&gt;protobuf&lt;/code&gt; 实现加载和存储，包括 CPU 与 GPU 之间的无缝切换，都是通过配置来实现的，不需要通过硬编码的方式实现。&lt;/p&gt;

&lt;h3 id=&quot;网络初始化&quot;&gt;网络初始化&lt;/h3&gt;
&lt;p&gt;在&lt;code class=&quot;highlighter-rouge&quot;&gt;caffe.cpp&lt;/code&gt;中main函数之外通过&lt;code class=&quot;highlighter-rouge&quot;&gt;RegisterBrewFunction&lt;/code&gt;这个宏在每一个实现主要功能的函数之后将这个函数的名字和其对应的函数指针添加到了&lt;code class=&quot;highlighter-rouge&quot;&gt;g_brew_map&lt;/code&gt;中,具体分别为&lt;code class=&quot;highlighter-rouge&quot;&gt;train()&lt;/code&gt;，&lt;code class=&quot;highlighter-rouge&quot;&gt;test()&lt;/code&gt;，&lt;code class=&quot;highlighter-rouge&quot;&gt;device_query()&lt;/code&gt;，&lt;code class=&quot;highlighter-rouge&quot;&gt;time()&lt;/code&gt;这四个函数。&lt;/p&gt;

&lt;p&gt;在运行的时候,根据传入的参数在main函数中，通过&lt;code class=&quot;highlighter-rouge&quot;&gt;GetBrewFunction&lt;/code&gt;得到了我们需要调用的那个函数的函数指针，并完成了调用。&lt;/p&gt;

&lt;p&gt;在我们上面所说的训练LeNet的例子中，传入的第二个参数为&lt;code class=&quot;highlighter-rouge&quot;&gt;train&lt;/code&gt;，所以调用的函数为&lt;code class=&quot;highlighter-rouge&quot;&gt;caffe.cpp&lt;/code&gt;中的&lt;code class=&quot;highlighter-rouge&quot;&gt;int train()&lt;/code&gt;函数，接下来主要看这个函数的内容。在&lt;code class=&quot;highlighter-rouge&quot;&gt;train&lt;/code&gt;函数中有下面两行代码，下面的代码定义了一个指向&lt;code class=&quot;highlighter-rouge&quot;&gt;Solver&lt;/code&gt;的&lt;code class=&quot;highlighter-rouge&quot;&gt;shared_ptr&lt;/code&gt;。其中主要是通过调用&lt;code class=&quot;highlighter-rouge&quot;&gt;SolverRegistry&lt;/code&gt;这个类的静态成员函数&lt;code class=&quot;highlighter-rouge&quot;&gt;CreateSolver&lt;/code&gt;得到一个指向&lt;code class=&quot;highlighter-rouge&quot;&gt;Solver&lt;/code&gt;的指针来构造&lt;code class=&quot;highlighter-rouge&quot;&gt;shared_ptr&lt;/code&gt;类型的solver。而且由于C++多态的特性，尽管solver是一个指向基类Solver类型的指针，通过solver这个智能指针来调用各个成员函数会调用到各个子类(SGDSolver等)的函数。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;// caffe.cpp
// 其中输入参数solver_param就是上面所说的第三个参数：网络的模型及求解文件
shared_ptr&amp;lt;caffe::Solver&amp;lt;float&amp;gt; &amp;gt;
    solver(caffe::SolverRegistry&amp;lt;float&amp;gt;::CreateSolver(solver_param);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;从上面代码可以看出，会先调用父类&lt;code class=&quot;highlighter-rouge&quot;&gt;Solver&lt;/code&gt;的构造函数，如下所示。&lt;code class=&quot;highlighter-rouge&quot;&gt;Solver&lt;/code&gt;类的构造函数通过&lt;code class=&quot;highlighter-rouge&quot;&gt;Init(param)&lt;/code&gt;函数来初始化网络。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;//solver.cpp
template &amp;lt;typename Dtype&amp;gt;
Solver&amp;lt;Dtype&amp;gt;::Solver(const SolverParameter&amp;amp; param, const Solver* root_solver)
    : net_(), callbacks_(), root_solver_(root_solver),requested_early_exit_(false)
{
  Init(param);
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;而在&lt;code class=&quot;highlighter-rouge&quot;&gt;Init(paran)&lt;/code&gt;函数中，又主要是通过&lt;code class=&quot;highlighter-rouge&quot;&gt;InitTrainNet()&lt;/code&gt;和&lt;code class=&quot;highlighter-rouge&quot;&gt;InitTestNets()&lt;/code&gt;函数分别来搭建训练网络结构和测试网络结构。&lt;/p&gt;

&lt;p&gt;训练网络只能有一个,在&lt;code class=&quot;highlighter-rouge&quot;&gt;InitTrainNet()&lt;/code&gt;函数中首先会设置一些基本参数，包括设置网络的状态为TRAIN，确定训练网络只有一个等，然会会通过&lt;code class=&quot;highlighter-rouge&quot;&gt;net_.reset(new Net&amp;lt;Dtype&amp;gt;(net_param));&lt;/code&gt;这条语句新建了一个Net对象。&lt;code class=&quot;highlighter-rouge&quot;&gt;InitTestNets()&lt;/code&gt;函数和&lt;code class=&quot;highlighter-rouge&quot;&gt;InitTrainNet()&lt;/code&gt;函数基本类似。&lt;/p&gt;

&lt;p&gt;上面语句新建了&lt;code class=&quot;highlighter-rouge&quot;&gt;Net&lt;/code&gt;对象之后会调用&lt;code class=&quot;highlighter-rouge&quot;&gt;Net&lt;/code&gt;类的构造函数，构造函数是通过&lt;code class=&quot;highlighter-rouge&quot;&gt;Init(param)&lt;/code&gt;函数来初始化网络结构的。&lt;/p&gt;

&lt;p&gt;在&lt;code class=&quot;highlighter-rouge&quot;&gt;net.cpp&lt;/code&gt;里&lt;code class=&quot;highlighter-rouge&quot;&gt;init()&lt;/code&gt;的主要内容是：其中&lt;code class=&quot;highlighter-rouge&quot;&gt;LayerRegistry&amp;lt;Dtype&amp;gt;::CreateLayer(layer_param)&lt;/code&gt;主要是通过调用&lt;code class=&quot;highlighter-rouge&quot;&gt;LayerRegistry&lt;/code&gt;这个类的静态成员函数&lt;code class=&quot;highlighter-rouge&quot;&gt;CreateLayer&lt;/code&gt;得到一个指向&lt;code class=&quot;highlighter-rouge&quot;&gt;Layer&lt;/code&gt;类的&lt;code class=&quot;highlighter-rouge&quot;&gt;shared_ptr&lt;/code&gt;类型指针。并把每一层的指针存放在&lt;code class=&quot;highlighter-rouge&quot;&gt;vector&amp;lt;shared_ptr&amp;lt;Layer&amp;lt;Dtype&amp;gt; &amp;gt; &amp;gt; layers_&lt;/code&gt;这个指针容器里。这里相当于根据每层的参数&lt;code class=&quot;highlighter-rouge&quot;&gt;layer_param&lt;/code&gt;实例化了对应的各个子类层，比如&lt;code class=&quot;highlighter-rouge&quot;&gt;conv_layer&lt;/code&gt;(卷积层)和&lt;code class=&quot;highlighter-rouge&quot;&gt;pooling_layer&lt;/code&gt;(池化层)。实例化了各层就会调用每个层的构造函数，但每层的构造函数都没有做什么大的设置。&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;init()&lt;/code&gt;函数主要又四个部分组成：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;AppendBottom：设置每一层的输入数据&lt;/li&gt;
  &lt;li&gt;AppendTop：设置每一层的输出数据&lt;/li&gt;
  &lt;li&gt;layers_[layer_id]-&amp;gt;SetUp：对上面设置的输入输出数据计算分配空间，并设置每层的可学习参数(权值和偏置)&lt;/li&gt;
  &lt;li&gt;AppendParam：对上面申请的可学习参数进行设置，主要包括学习率和正则率等。&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;//net.cpp Init()
for (int layer_id = 0; layer_id &amp;lt; param.layer_size(); ++layer_id) {//param是网络参数，layer_size()返回网络拥有的层数
    const LayerParameter&amp;amp; layer_param = param.layer(layer_id);//获取当前layer的参数
    layers_.push_back(LayerRegistry&amp;lt;Dtype&amp;gt;::CreateLayer(layer_param));//根据参数实例化layer


//下面的两个for循环将此layer的bottom blob的指针和top blob的指针放入bottom_vecs_和top_vecs_,bottom blob和top blob的实例全都存放在blobs_中。相邻的两层，前一层的top blob是后一层的bottom blob，所以blobs_的同一个blob既可能是bottom blob，也可能使top blob。
    for (int bottom_id = 0; bottom_id &amp;lt; layer_param.bottom_size();++bottom_id) {
       const int blob_id=AppendBottom(param,layer_id,bottom_id,&amp;amp;available_blobs,&amp;amp;blob_name_to_idx);
    }

    for (int top_id = 0; top_id &amp;lt; num_top; ++top_id) {
       AppendTop(param, layer_id, top_id, &amp;amp;available_blobs, &amp;amp;blob_name_to_idx);
    }

// 调用layer类的Setup函数进行初始化，输入参数：每个layer的输入blobs以及输出blobs,为每个blob设置大小
layers_[layer_id]-&amp;gt;SetUp(bottom_vecs_[layer_id], top_vecs_[layer_id]);

//接下来的工作是将每层的parameter的指针塞进params_，尤其是learnable_params_。
   const int num_param_blobs = layers_[layer_id]-&amp;gt;blobs().size();
   for (int param_id = 0; param_id &amp;lt; num_param_blobs; ++param_id) {
       AppendParam(param, layer_id, param_id);
       //AppendParam负责具体的dirtywork
    }


    }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;经过上面的过程，&lt;code class=&quot;highlighter-rouge&quot;&gt;Net&lt;/code&gt;类的初始化工作基本就完成了。总体的流程大概就是新建一个&lt;code class=&quot;highlighter-rouge&quot;&gt;Solver&lt;/code&gt;对象，然后调用&lt;code class=&quot;highlighter-rouge&quot;&gt;Solver&lt;/code&gt;类的构造函数，然后在&lt;code class=&quot;highlighter-rouge&quot;&gt;Solver&lt;/code&gt;的构造函数中又会新建&lt;code class=&quot;highlighter-rouge&quot;&gt;Net&lt;/code&gt;类实例，在&lt;code class=&quot;highlighter-rouge&quot;&gt;Net&lt;/code&gt;类的构造函数中又会新建各个&lt;code class=&quot;highlighter-rouge&quot;&gt;Layer&lt;/code&gt;的实例,一直具体到设置每个&lt;code class=&quot;highlighter-rouge&quot;&gt;Blob&lt;/code&gt;,大概就介绍完了网络初始化的工作。&lt;/p&gt;

&lt;h3 id=&quot;训练过程&quot;&gt;训练过程&lt;/h3&gt;
&lt;p&gt;完成初始化之后，就可以开始对网络经行训练了，开始训练的代码如下所示，指向Solver类的指针solver开始调用Solver类的成员函数Solve()，名称比较绕啊。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;// 开始优化
solver-&amp;gt;Solve();
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Solve&lt;/code&gt;函数其实主要就是调用了&lt;code class=&quot;highlighter-rouge&quot;&gt;Solver&lt;/code&gt;的另一个成员函数&lt;code class=&quot;highlighter-rouge&quot;&gt;Step()&lt;/code&gt;来完成实际的迭代训练过程。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;//solver.cpp
template &amp;lt;typename Dtype&amp;gt;
void Solver&amp;lt;Dtype&amp;gt;::Solve(const char* resume_file) {
  ...
  int start_iter = iter_;
  ...
  // 然后调用了'Step'函数，这个函数执行了实际的逐步的迭代过程
  Step(param_.max_iter() - iter_);
  ...
  LOG(INFO) &amp;lt;&amp;lt; &quot;Optimization Done.&quot;;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;顺着来看看这个&lt;code class=&quot;highlighter-rouge&quot;&gt;Step()&lt;/code&gt;函数的主要代码,首先是一个大循环设置了总的迭代次数，在每次迭代中训练iter_size x batch_size个样本，这个设置是为了在GPU的显存不够的时候使用，比如我本来想把batch_size设置为128，iter_size是默认为1的，但是会&lt;code class=&quot;highlighter-rouge&quot;&gt;out_of_memory&lt;/code&gt;，借助这个方法，可以设置batch_size=32，iter_size=4，那实际上每次迭代还是处理了128个数据。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;//solver.cpp
template &amp;lt;typename Dtype&amp;gt;
void Solver&amp;lt;Dtype&amp;gt;::Step(int iters) {
  ...
  //迭代
  while (iter_ &amp;lt; stop_iter) {
    ...
    // iter_size也是在solver.prototxt里设置，实际上的batch_size=iter_size*网络定义里的batch_size，
    // 因此每一次迭代的loss是iter_size次迭代的和，再除以iter_size，这个loss是通过调用`Net::ForwardBackward`函数得到的
    // accumulate gradients over `iter_size` x `batch_size` instances
    for (int i = 0; i &amp;lt; param_.iter_size(); ++i) {
    /*
     * 调用了Net中的代码，主要完成了前向后向的计算，
     * 前向用于计算模型的最终输出和Loss，后向用于
     * 计算每一层网络和参数的梯度。
     */
      loss += net_-&amp;gt;ForwardBackward();
    }

    ...

    /*
     * 这个函数主要做Loss的平滑。由于Caffe的训练方式是SGD，我们无法把所有的数据同时
     * 放入模型进行训练，那么部分数据产生的Loss就可能会和全样本的平均Loss不同，在必要
     * 时候将Loss和历史过程中更新的Loss求平均就可以减少Loss的震荡问题。
     */
    UpdateSmoothedLoss(loss, start_iter, average_loss);


    ...
    // 执行梯度的更新，这个函数在基类`Solver`中没有实现，会调用每个子类自己的实现
    //，后面具体分析`SGDSolver`的实现
    ApplyUpdate();

    // 迭代次数加1
    ++iter_;
    ...

  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;上面&lt;code class=&quot;highlighter-rouge&quot;&gt;Step()&lt;/code&gt;函数主要分为三部分：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;loss += net_-&amp;gt;ForwardBackward();
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;这行代码通过&lt;code class=&quot;highlighter-rouge&quot;&gt;Net&lt;/code&gt;类的&lt;code class=&quot;highlighter-rouge&quot;&gt;net_&lt;/code&gt;指针调用其成员函数&lt;code class=&quot;highlighter-rouge&quot;&gt;ForwardBackward()&lt;/code&gt;，其代码如下所示,分别调用了成员函数&lt;code class=&quot;highlighter-rouge&quot;&gt;Forward(&amp;amp;loss)&lt;/code&gt;和成员函数&lt;code class=&quot;highlighter-rouge&quot;&gt;Backward()&lt;/code&gt;来进行前向传播和反向传播。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;// net.hpp
// 进行一次正向传播，一次反向传播
Dtype ForwardBackward() {
  Dtype loss;
  Forward(&amp;amp;loss);
  Backward();
  return loss;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;前面的&lt;code class=&quot;highlighter-rouge&quot;&gt;Forward(&amp;amp;loss)&lt;/code&gt;函数最终会执行到下面一段代码,&lt;code class=&quot;highlighter-rouge&quot;&gt;Net&lt;/code&gt;类的&lt;code class=&quot;highlighter-rouge&quot;&gt;Forward()&lt;/code&gt;函数会对网络中的每一层执行&lt;code class=&quot;highlighter-rouge&quot;&gt;Layer&lt;/code&gt;类的成员函数&lt;code class=&quot;highlighter-rouge&quot;&gt;Forward()&lt;/code&gt;，而具体的每一层&lt;code class=&quot;highlighter-rouge&quot;&gt;Layer&lt;/code&gt;的派生类会重写&lt;code class=&quot;highlighter-rouge&quot;&gt;Forward()&lt;/code&gt;函数来实现不同层的前向计算功能。上面的&lt;code class=&quot;highlighter-rouge&quot;&gt;Backward()&lt;/code&gt;反向求导函数也和&lt;code class=&quot;highlighter-rouge&quot;&gt;Forward()&lt;/code&gt;类似，调用不同层的&lt;code class=&quot;highlighter-rouge&quot;&gt;Backward()&lt;/code&gt;函数来计算每层的梯度。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;//net.cpp
for (int i = start; i &amp;lt;= end; ++i) {
// 对每一层进行前向计算，返回每层的loss，其实只有最后一层loss不为0
  Dtype layer_loss = layers_[i]-&amp;gt;Forward(bottom_vecs_[i], top_vecs_[i]);
  loss += layer_loss;
  if (debug_info_) { ForwardDebugInfo(i); }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;UpdateSmoothedLoss();
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;这个函数主要做&lt;code class=&quot;highlighter-rouge&quot;&gt;Loss&lt;/code&gt;的平滑。由于Caffe的训练方式是SGD，我们无法把所有的数据同时放入模型进行训练，那么部分数据产生的&lt;code class=&quot;highlighter-rouge&quot;&gt;Loss&lt;/code&gt;就可能会和全样本的平均&lt;code class=&quot;highlighter-rouge&quot;&gt;Loss&lt;/code&gt;不同，在必要时候将&lt;code class=&quot;highlighter-rouge&quot;&gt;Loss&lt;/code&gt;和历史过程中更新的&lt;code class=&quot;highlighter-rouge&quot;&gt;Loss&lt;/code&gt;求平均就可以减少&lt;code class=&quot;highlighter-rouge&quot;&gt;Loss&lt;/code&gt;的震荡问题&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ApplyUpdate();
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;这个函数是&lt;code class=&quot;highlighter-rouge&quot;&gt;Solver&lt;/code&gt;类的纯虚函数，需要派生类来实现，比如SGDSolver类实现的&lt;code class=&quot;highlighter-rouge&quot;&gt;ApplyUpdate()&lt;/code&gt;;函数如下，主要内容包括：设置参数的学习率；对梯度进行Normalize；对反向求导得到的梯度添加正则项的梯度；最后根据SGD算法计算最终的梯度；最后的最后把计算得到的最终梯度对权值进行更新。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;template &amp;lt;typename Dtype&amp;gt;
void SGDSolver&amp;lt;Dtype&amp;gt;::ApplyUpdate() {
  CHECK(Caffe::root_solver());

  // GetLearningRate根据设置的lr_policy来计算当前迭代的learning rate的值
  Dtype rate = GetLearningRate();

  // 判断是否需要输出当前的learning rate
  if (this-&amp;gt;param_.display() &amp;amp;&amp;amp; this-&amp;gt;iter_ % this-&amp;gt;param_.display() == 0) {
    LOG(INFO) &amp;lt;&amp;lt; &quot;Iteration &quot; &amp;lt;&amp;lt; this-&amp;gt;iter_ &amp;lt;&amp;lt; &quot;, lr = &quot; &amp;lt;&amp;lt; rate;
  }

  // 避免梯度爆炸，如果梯度的二范数超过了某个数值则进行scale操作，将梯度减小
  ClipGradients();

  // 对所有可更新的网络参数进行操作
  for (int param_id = 0; param_id &amp;lt; this-&amp;gt;net_-&amp;gt;learnable_params().size();
       ++param_id) {
	// 将第param_id个参数的梯度除以iter_size，
	// 这一步的作用是保证实际的batch_size=iter_size*设置的batch_size
    Normalize(param_id);

    // 将正则化部分的梯度降入到每个参数的梯度中
    Regularize(param_id);

    // 计算SGD算法的梯度(momentum等)
    ComputeUpdateValue(param_id, rate);
  }
  // 调用`Net::Update`更新所有的参数
  this-&amp;gt;net_-&amp;gt;Update();
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;等进行了所有的循环，网络的训练也算是完成了。上面大概说了下使用Caffe进行网络训练时网络初始化以及前向传播、反向传播、梯度更新的过程，其中省略了大量的细节。上面还有很多东西都没提到，比如说Caffe中&lt;code class=&quot;highlighter-rouge&quot;&gt;Layer&lt;/code&gt;派生类的注册及各个具体层前向反向的实现、&lt;code class=&quot;highlighter-rouge&quot;&gt;Solver&lt;/code&gt;派生类的注册、网络结构的读取、模型的保存等等大量内容。&lt;/p&gt;</content><author><name>Jim Chen</name></author><summary type="html">之前一直在学习Transfer Learning，看到DAN和RTN的源代码都是用Caffe写的，所以趁此机会来学习一下Caffe的源代码，为了之后能自己写出来自己的类。</summary></entry><entry><title type="html">From object recognition to instance recognition</title><link href="http://localhost:4000/posts/2017-06-25-From-object-recognition-to-instance-recognition" rel="alternate" type="text/html" title="From object recognition to instance recognition" /><published>2017-06-25T20:38:00+08:00</published><updated>2017-06-25T20:38:00+08:00</updated><id>http://localhost:4000/posts/From-object-recognition-to-instance-recognition</id><content type="html" xml:base="http://localhost:4000/posts/2017-06-25-From-object-recognition-to-instance-recognition">&lt;p&gt;最近考完试重新开始学习物体识别相关的知识。之前大作业的时候就学习过RCNN系列的三篇论文，然后自己尝试了基于SegNet的semantic segmentation任务。前段时间看Kaiming大神 的Mask R-CNN，又想重新温故一下这一系列的文章，正好也是第一次在知乎上写文章，就当作给自己看的论文的一个总结的地方吧。（反正也没人看，我就随便写写了）
先推荐一个之前在知乎上看的一个专栏（&lt;a href=&quot;https://zhuanlan.zhihu.com/xiaoleimlnote&quot;&gt;晓雷机器学习笔记&lt;/a&gt;）的这个系列，感觉写的也很不错，所以放在这里做个列表：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/23006190?refer=xiaoleimlnote&quot;&gt;R-CNN&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/24780395?refer=xiaoleimlnote&quot;&gt;Fast R-CNN&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/24916624?refer=xiaoleimlnote&quot;&gt;Faster R-CNN&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/24774302?refer=xiaoleimlnote&quot;&gt;SPPNet&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/24916786?refer=xiaoleimlnote&quot;&gt;YOLO&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/24954433?refer=xiaoleimlnote&quot;&gt;SSD&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/25167153?refer=xiaoleimlnote&quot;&gt;YOLO2&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;之后应该是直接看到了Mask R-CNN这篇文章，附上Tensorflow版的代码地址（&lt;a href=&quot;https://github.com/CharlesShang/FastMaskRCNN&quot;&gt;CharlesShang/FastMaskRCNN&lt;/a&gt;），近期准备看看源码学习一下。&lt;/p&gt;</content><author><name>Jim Chen</name></author><summary type="html">最近考完试重新开始学习物体识别相关的知识。之前大作业的时候就学习过RCNN系列的三篇论文，然后自己尝试了基于SegNet的semantic segmentation任务。前段时间看Kaiming大神 的Mask R-CNN，又想重新温故一下这一系列的文章，正好也是第一次在知乎上写文章，就当作给自己看的论文的一个总结的地方吧。（反正也没人看，我就随便写写了） 先推荐一个之前在知乎上看的一个专栏（晓雷机器学习笔记）的这个系列，感觉写的也很不错，所以放在这里做个列表：</summary></entry></feed>