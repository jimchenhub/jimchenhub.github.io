---
layout: post
title:  "FPN&DSSD论文笔记"
date:   2018-04-10 22:00:00 +0800
categories: Object Detection, FPN，DSSD, Deep Learning

---

## FPN

我们知道**低层的特征语义信息比较少，但是目标位置准确；高层的特征语义信息比较丰富，但是目标位置比较粗略。另外虽然也有些算法采用多尺度特征融合的方式，但是一般是采用融合后的特征做预测，而本文不一样的地方在于预测是在不同特征层独立进行的。**

SSD、MSCNN 利用不同层的特征图进行不同尺寸的目标预测，又快又好，基于proposal的方法也借鉴了这个思路：在不同深度的特征层上预测不同尺寸的目标。

但是SSD为了预测小目标，就得把比较低的层拿出来预测，这样的话就很难保证有很强的语义特征，所以作者就想着，为什么不把高层的特征再传下来，补充低层的语义，这样就可以获得高分辨率、强语义的特征，有利于小目标的检测。

### 网络结构

#### Pyramid scale learning

![](/images/FPN_Pyramid.jpg)

本文对过去的多种pyramid的操作进行了对比：

（a）图像金字塔，即将图像做成不同的scale，然后不同scale的图像生成对应的不同scale的特征。这种方法的缺点在于**增加了时间成本**。有些算法会在测试时候采用图像金字塔。 

（b）像SPP net，Fast RCNN，Faster RCNN是采用这种方式，即仅采用网络最后一层的特征。 

（c）像SSD（Single Shot Detector）采用这种多尺度特征融合的方式，没有上采样过程，即从网络不同层抽取不同尺度的特征做预测，这种方式不会增加额外的计算量。作者认为SSD算法中**没有用到足够低层的特征（在SSD中，最低层的特征是VGG网络的conv4_3）**，而在作者看来足够低层的特征对于检测小物体是很有帮助的。 

（d）本文作者是采用这种方式，顶层特征通过上采样和低层特征做融合，而且**每层都是独立预测**的。

#### Structure

![](/images/FPN_network.jpg)

整个网络包括了正常的Bottom-Up部分和在本文中增加的Top-Down结构。FPN的Top-Down结构把每一层的channels都改成了256d，然后每一层经过一个3x3的卷积消除上采样带来的混叠效应。

虚线框可以理解成特征融合的操作，1x1的卷积有三个作用（reference：[FPN + DSSD 阅读笔记](https://zhuanlan.zhihu.com/p/26743074)）：使bottom-up对应层降维至256；缓冲作用，防止梯度直接影响bottom-up主干网络，更稳定；组合特征。

上采样2x up作者采用的是nearest neighbor。加号是element-wise sum（DSSD中实验结果是element-wise product会好一点点）。

### 实验结果和分析

本文在RPN和Detection上都进行了测试，这里重点讲讲在Detection的测试中的一些小技巧。

*由于在做detection时，每一个level的层是单独预测的，而每一层的分辨率又不一样，那么一个图片上的RoI，具体应该分配给哪一个层来处理呢？*

一般的想法是直接按照尺寸来分配，但是作者采用了下面的公式：

![](/images/FPN_roi_assign.jpg)

Faster RCNN用了Res-Net的![C_{4} ](https://www.zhihu.com/equation?tex=C_%7B4%7D+)，所以作者也把![k_{0} ](https://www.zhihu.com/equation?tex=k_%7B0%7D+)设置为4；也就是说如果你的RoI的面积小于![224^{2} ](https://www.zhihu.com/equation?tex=224%5E%7B2%7D+)，那么这个RoI就从![C_{3} ](https://www.zhihu.com/equation?tex=C_%7B3%7D+)上提取特征。

#### 分析

- 如果没有top-down的语义增强分支（仍然从不同的层输出），那么RPN的AR（average recall）会下降6%左右；
- 如果不进行特征的融合（也就是说去掉所有的1x1侧连接），虽然理论上分辨率没变，语义也增强了，但是AR下降了10%左右！作者认为这些特征上下采样太多次了，导致它们不适于定位。**Bottom-up的特征包含了更精确的位置信息。**
- 如果不利用多个层进行输出呢？作者尝试只在top-down的最后一层（分辨率最高、语义最强）设置anchors，仍然比FPN低了5%。需要注意的是这时的**anchors多了很多，但是并没有提高AR。**
- 在RPN和object detection任务中，FPN中每一层的heads 参数都是共享的，作者认为**共享参数的效果也不错就说明FPN中所有层的语义都相似。**